{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/tut1.html\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sebastian\n",
      "[nltk_data]     Birk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sebastian\n",
      "[nltk_data]     Birk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords and lemmatizer from nltk package\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log events\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data with timestamp as index\n",
    "tweets = pd.read_csv(\"tweets.csv\", encoding=\"latin1\", parse_dates=True, \n",
    "                     index_col=\"created\", usecols=range(1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>tweetcount</th>\n",
       "      <th>movement</th>\n",
       "      <th>language3</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weeknumber</th>\n",
       "      <th>month</th>\n",
       "      <th>idBarrio_xy</th>\n",
       "      <th>idBarrio</th>\n",
       "      <th>user</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-18 19:12:28</th>\n",
       "      <td>Brew Pub to try a few of the 30 beers on offer...</td>\n",
       "      <td>Brew Pub to try a few of the 30 beers on offer...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.169180</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>46</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u03885</td>\n",
       "      <td>[#olgodbarcelona, #ølgod, #triathlontraining]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 10:46:20</th>\n",
       "      <td>\"Art is coming face to face with yourself\" &lt;ed...</td>\n",
       "      <td>Art is coming face to face with yourself   Par...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.171902</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Monday</td>\n",
       "      <td>48</td>\n",
       "      <td>November</td>\n",
       "      <td>11</td>\n",
       "      <td>55</td>\n",
       "      <td>u00395</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 20:44:52</th>\n",
       "      <td>Today's quickie @ Parc de les Tres Xemeneies h...</td>\n",
       "      <td>Today's quickie  Parc de les Tres Xemeneies</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.171902</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>11</td>\n",
       "      <td>55</td>\n",
       "      <td>u00395</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23 10:47:18</th>\n",
       "      <td>All hail the freshest seafood in town. Let's f...</td>\n",
       "      <td>All hail the freshest seafood in town Let's fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.172170</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u01268</td>\n",
       "      <td>[#hadthemostmusselsinmylife, #tripoftheyear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 22:30:26</th>\n",
       "      <td>Dance to the beat of your own drum.\\n\\n3&lt;U+20E...</td>\n",
       "      <td>Dance to the beat of your own drum\\n\\n3\\n\\n#ur...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.171902</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>46</td>\n",
       "      <td>November</td>\n",
       "      <td>11</td>\n",
       "      <td>55</td>\n",
       "      <td>u00495</td>\n",
       "      <td>[#urban, #music, #live, #stage, #graffiti, #st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "created                                                                  \n",
       "2017-11-18 19:12:28  Brew Pub to try a few of the 30 beers on offer...   \n",
       "2017-11-27 10:46:20  \"Art is coming face to face with yourself\" <ed...   \n",
       "2017-11-25 20:44:52  Today's quickie @ Parc de les Tres Xemeneies h...   \n",
       "2017-11-23 10:47:18  All hail the freshest seafood in town. Let's f...   \n",
       "2017-11-19 22:30:26  Dance to the beat of your own drum.\\n\\n3<U+20E...   \n",
       "\n",
       "                                                            text_clean  \\\n",
       "created                                                                  \n",
       "2017-11-18 19:12:28  Brew Pub to try a few of the 30 beers on offer...   \n",
       "2017-11-27 10:46:20  Art is coming face to face with yourself   Par...   \n",
       "2017-11-25 20:44:52       Today's quickie  Parc de les Tres Xemeneies    \n",
       "2017-11-23 10:47:18  All hail the freshest seafood in town Let's fe...   \n",
       "2017-11-19 22:30:26  Dance to the beat of your own drum\\n\\n3\\n\\n#ur...   \n",
       "\n",
       "                     favoriteCount replyToSN  truncated  replyToSID  \\\n",
       "created                                                               \n",
       "2017-11-18 19:12:28              0       NaN      False         NaN   \n",
       "2017-11-27 10:46:20              0       NaN      False         NaN   \n",
       "2017-11-25 20:44:52              2       NaN      False         NaN   \n",
       "2017-11-23 10:47:18              0       NaN      False         NaN   \n",
       "2017-11-19 22:30:26              0       NaN      False         NaN   \n",
       "\n",
       "                     replyToUID  \\\n",
       "created                           \n",
       "2017-11-18 19:12:28         NaN   \n",
       "2017-11-27 10:46:20         NaN   \n",
       "2017-11-25 20:44:52         NaN   \n",
       "2017-11-23 10:47:18         NaN   \n",
       "2017-11-19 22:30:26         NaN   \n",
       "\n",
       "                                                          statusSource  \\\n",
       "created                                                                  \n",
       "2017-11-18 19:12:28  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-27 10:46:20  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-25 20:44:52  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-23 10:47:18  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-19 22:30:26  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "\n",
       "                     retweetCount  longitude  \\\n",
       "created                                        \n",
       "2017-11-18 19:12:28             0   2.169180   \n",
       "2017-11-27 10:46:20             0   2.171902   \n",
       "2017-11-25 20:44:52             0   2.171902   \n",
       "2017-11-23 10:47:18             0   2.172170   \n",
       "2017-11-19 22:30:26             0   2.171902   \n",
       "\n",
       "                                           ...                          \\\n",
       "created                                    ...                           \n",
       "2017-11-18 19:12:28                        ...                           \n",
       "2017-11-27 10:46:20                        ...                           \n",
       "2017-11-25 20:44:52                        ...                           \n",
       "2017-11-23 10:47:18                        ...                           \n",
       "2017-11-19 22:30:26                        ...                           \n",
       "\n",
       "                     tweetcount  movement language3  dayofweek weeknumber  \\\n",
       "created                                                                     \n",
       "2017-11-18 19:12:28           1  1.000000   ENGLISH   Saturday         46   \n",
       "2017-11-27 10:46:20           7  0.285714   ENGLISH     Monday         48   \n",
       "2017-11-25 20:44:52           7  0.285714   ENGLISH   Saturday         47   \n",
       "2017-11-23 10:47:18           3  0.666667   ENGLISH   Thursday         47   \n",
       "2017-11-19 22:30:26           6  0.500000   ENGLISH     Sunday         46   \n",
       "\n",
       "                        month  idBarrio_xy  idBarrio    user  \\\n",
       "created                                                        \n",
       "2017-11-18 19:12:28  November            1        55  u03885   \n",
       "2017-11-27 10:46:20  November           11        55  u00395   \n",
       "2017-11-25 20:44:52  November           11        55  u00395   \n",
       "2017-11-23 10:47:18  November            1        55  u01268   \n",
       "2017-11-19 22:30:26  November           11        55  u00495   \n",
       "\n",
       "                                                              hashtags  \n",
       "created                                                                 \n",
       "2017-11-18 19:12:28      [#olgodbarcelona, #ølgod, #triathlontraining]  \n",
       "2017-11-27 10:46:20                                                 []  \n",
       "2017-11-25 20:44:52                                                 []  \n",
       "2017-11-23 10:47:18       [#hadthemostmusselsinmylife, #tripoftheyear]  \n",
       "2017-11-19 22:30:26  [#urban, #music, #live, #stage, #graffiti, #st...  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataframe\n",
    "english_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 7633 entries, 2017-11-18 19:12:28 to 2017-06-11 15:55:42\n",
      "Data columns (total 28 columns):\n",
      "text                    7633 non-null object\n",
      "text_clean              7633 non-null object\n",
      "favoriteCount           7633 non-null int64\n",
      "replyToSN               150 non-null object\n",
      "truncated               7633 non-null bool\n",
      "replyToSID              87 non-null float64\n",
      "replyToUID              150 non-null float64\n",
      "statusSource            7633 non-null object\n",
      "retweetCount            7633 non-null int64\n",
      "longitude               7633 non-null float64\n",
      "latitude                7633 non-null float64\n",
      "id_seccion              7633 non-null int64\n",
      "horaPeticion            7633 non-null object\n",
      "id_distrito             7633 non-null int64\n",
      "grupoHora               7633 non-null object\n",
      "id_seccion_xy           7633 non-null int64\n",
      "favoriteCountOutlier    7633 non-null int64\n",
      "retweetCountOutlier     7633 non-null int64\n",
      "tweetcount              7633 non-null int64\n",
      "movement                7633 non-null float64\n",
      "language3               7633 non-null object\n",
      "dayofweek               7633 non-null object\n",
      "weeknumber              7633 non-null int64\n",
      "month                   7633 non-null object\n",
      "idBarrio_xy             7633 non-null int64\n",
      "idBarrio                7633 non-null int64\n",
      "user                    7633 non-null object\n",
      "hashtags                7633 non-null object\n",
      "dtypes: bool(1), float64(5), int64(11), object(11)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# display dataframe info\n",
    "english_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>id_seccion</th>\n",
       "      <th>id_distrito</th>\n",
       "      <th>id_seccion_xy</th>\n",
       "      <th>favoriteCountOutlier</th>\n",
       "      <th>retweetCountOutlier</th>\n",
       "      <th>tweetcount</th>\n",
       "      <th>movement</th>\n",
       "      <th>weeknumber</th>\n",
       "      <th>idBarrio_xy</th>\n",
       "      <th>idBarrio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23778.000000</td>\n",
       "      <td>5.740000e+02</td>\n",
       "      <td>8.210000e+02</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>2.377800e+04</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>2.377800e+04</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.200774</td>\n",
       "      <td>9.015351e+17</td>\n",
       "      <td>4.078693e+16</td>\n",
       "      <td>0.285222</td>\n",
       "      <td>2.171064</td>\n",
       "      <td>41.395325</td>\n",
       "      <td>8.019040e+08</td>\n",
       "      <td>801903.985953</td>\n",
       "      <td>7.999476e+08</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>17.686349</td>\n",
       "      <td>0.690218</td>\n",
       "      <td>34.522962</td>\n",
       "      <td>18.136008</td>\n",
       "      <td>21.180503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.403217</td>\n",
       "      <td>5.293076e+16</td>\n",
       "      <td>1.808261e+17</td>\n",
       "      <td>4.610914</td>\n",
       "      <td>0.021190</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>3.028198e+03</td>\n",
       "      <td>3.023254</td>\n",
       "      <td>3.955736e+07</td>\n",
       "      <td>0.025932</td>\n",
       "      <td>0.011232</td>\n",
       "      <td>62.044925</td>\n",
       "      <td>0.326073</td>\n",
       "      <td>7.046783</td>\n",
       "      <td>21.415965</td>\n",
       "      <td>22.107374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.229677e+17</td>\n",
       "      <td>7.802900e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.059243</td>\n",
       "      <td>41.332580</td>\n",
       "      <td>8.019010e+08</td>\n",
       "      <td>801901.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.862349e+17</td>\n",
       "      <td>1.195479e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.159720</td>\n",
       "      <td>41.382780</td>\n",
       "      <td>8.019020e+08</td>\n",
       "      <td>801902.000000</td>\n",
       "      <td>8.019020e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.079774e+17</td>\n",
       "      <td>3.537928e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.174778</td>\n",
       "      <td>41.395250</td>\n",
       "      <td>8.019021e+08</td>\n",
       "      <td>801902.000000</td>\n",
       "      <td>8.019021e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.232741e+17</td>\n",
       "      <td>1.028215e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.176944</td>\n",
       "      <td>41.404080</td>\n",
       "      <td>8.019060e+08</td>\n",
       "      <td>801906.000000</td>\n",
       "      <td>8.019050e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2449.000000</td>\n",
       "      <td>9.354775e+17</td>\n",
       "      <td>9.290872e+17</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>2.226620</td>\n",
       "      <td>41.465590</td>\n",
       "      <td>8.019102e+08</td>\n",
       "      <td>801910.000000</td>\n",
       "      <td>8.019102e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       favoriteCount    replyToSID    replyToUID  retweetCount     longitude  \\\n",
       "count   23778.000000  5.740000e+02  8.210000e+02  23778.000000  23778.000000   \n",
       "mean        1.200774  9.015351e+17  4.078693e+16      0.285222      2.171064   \n",
       "std        20.403217  5.293076e+16  1.808261e+17      4.610914      0.021190   \n",
       "min         0.000000  1.229677e+17  7.802900e+05      0.000000      2.059243   \n",
       "25%         0.000000  8.862349e+17  1.195479e+08      0.000000      2.159720   \n",
       "50%         0.000000  9.079774e+17  3.537928e+08      0.000000      2.174778   \n",
       "75%         1.000000  9.232741e+17  1.028215e+09      0.000000      2.176944   \n",
       "max      2449.000000  9.354775e+17  9.290872e+17    567.000000      2.226620   \n",
       "\n",
       "           latitude    id_seccion    id_distrito  id_seccion_xy  \\\n",
       "count  23778.000000  2.377800e+04   23778.000000   2.377800e+04   \n",
       "mean      41.395325  8.019040e+08  801903.985953   7.999476e+08   \n",
       "std        0.014936  3.028198e+03       3.023254   3.955736e+07   \n",
       "min       41.332580  8.019010e+08  801901.000000   0.000000e+00   \n",
       "25%       41.382780  8.019020e+08  801902.000000   8.019020e+08   \n",
       "50%       41.395250  8.019021e+08  801902.000000   8.019021e+08   \n",
       "75%       41.404080  8.019060e+08  801906.000000   8.019050e+08   \n",
       "max       41.465590  8.019102e+08  801910.000000   8.019102e+08   \n",
       "\n",
       "       favoriteCountOutlier  retweetCountOutlier    tweetcount      movement  \\\n",
       "count          23778.000000         23778.000000  23778.000000  23778.000000   \n",
       "mean               0.000673             0.000126     17.686349      0.690218   \n",
       "std                0.025932             0.011232     62.044925      0.326073   \n",
       "min                0.000000             0.000000      1.000000      0.100000   \n",
       "25%                0.000000             0.000000      1.000000      0.384615   \n",
       "50%                0.000000             0.000000      3.000000      0.750000   \n",
       "75%                0.000000             0.000000      8.000000      1.000000   \n",
       "max                1.000000             1.000000    440.000000      1.000000   \n",
       "\n",
       "         weeknumber   idBarrio_xy      idBarrio  \n",
       "count  23778.000000  23778.000000  23778.000000  \n",
       "mean      34.522962     18.136008     21.180503  \n",
       "std        7.046783     21.415965     22.107374  \n",
       "min       23.000000      0.000000      1.000000  \n",
       "25%       28.000000      6.000000      6.000000  \n",
       "50%       34.000000      7.000000      9.000000  \n",
       "75%       40.000000     25.000000     31.000000  \n",
       "max       49.000000     73.000000     73.000000  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe dataframe \n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide dataset according to language: extract english language\n",
    "english_tweets = tweets[tweets[\"language3\"] == \"ENGLISH\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess twitter text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove links\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text\"].str.replace(r\"http\\S+\", \"\")\n",
    "# remove emoticons\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"<.*>\", \"\")\n",
    "# remove @s\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"@\", \"\")\n",
    "# remove punctuation and special characters\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"&amp\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\.\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\,\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\;\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\-\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "english_tweets.columns\n",
    "cols = ['text', 'text_clean', 'favoriteCount', 'replyToSN', 'truncated', 'replyToSID',\n",
    "       'replyToUID', 'statusSource', 'retweetCount', 'longitude', 'latitude',\n",
    "       'id_seccion', 'horaPeticion', 'id_distrito', 'grupoHora',\n",
    "       'id_seccion_xy', 'favoriteCountOutlier', 'retweetCountOutlier',\n",
    "       'tweetcount', 'movement', 'language3', 'dayofweek', 'weeknumber',\n",
    "       'month', 'idBarrio_xy', 'idBarrio', 'user']\n",
    "\n",
    "english_tweets = english_tweets[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to train the topic models, 3 different pooling methods for the creation of documents are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 1 (No Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat every tweet as a document (no pooling)\n",
    "documents = english_tweets[\"text_clean\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 2 (User Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat all tweets by one user as a document (user pooling)\n",
    "user_combined = english_tweets[[\"text_clean\",\"user\"]].groupby(\"user\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_user_pooling = user_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 3 (Hashtag Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat all tweets with same hashtags as a document (hashtag pooling)\n",
    "\n",
    "# find all hashtags\n",
    "english_tweets[\"hashtags\"] = english_tweets[\"text_clean\"].str.findall(r'#.*?(?=\\s|$)')\n",
    "\n",
    "# separate hashtags in columns\n",
    "hashtags_tweets = pd.DataFrame(english_tweets[\"hashtags\"].tolist(),\n",
    "                               columns=[\"hashtag1\", \"hashtag2\", \"hashtag3\", \"hashtag4\",\n",
    "                                        \"hashtag5\", \"hashtag6\", \"hashtag7\", \"hashtag8\",\n",
    "                                        \"hashtag9\", \"hashtag10\", \"hashtag11\", \"hashtag12\",\n",
    "                                        \"hashtag13\"])\n",
    "\n",
    "# join hashtags with tweet text\n",
    "hashtags_tweets.index = english_tweets.index\n",
    "hashtags_tweets = english_tweets.join(hashtags_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one dataframe with text for each hashtag column and save them in a dictionary\n",
    "dict = {}\n",
    "for index, item in enumerate([\"hash1\", \"hash2\", \"hash3\", \"hash4\", \"hash5\",\n",
    "                              \"hash6\", \"hash7\", \"hash8\", \"hash9\", \"hash10\",\n",
    "                              \"hash11\", \"hash12\", \"hash13\"]):\n",
    "    dict[item] = hashtags_tweets[[\"hashtag\" + str(index + 1), \"text_clean\"]].copy()\n",
    "    dict[item].columns = [\"hashtag\", \"text\"]\n",
    "    dict[item].dropna(inplace=True)\n",
    "\n",
    "# concatenate all dataframes to one dataframe (the result is a dataframe\n",
    "# where there is text for each hashtag found)\n",
    "hashtags = pd.DataFrame()\n",
    "for item in dict:\n",
    "    hashtags = pd.concat([hashtags, dict[item]])\n",
    "\n",
    "# combine text for each hashtag\n",
    "hashtags_combined = hashtags.groupby(\"hashtag\")[\"text\"].apply(lambda x: \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove some generic hashtags that cover a lot of different topics\n",
    "hashtags_combined.drop([\"#Barcelona\", \"#Catalunya\", \"#Spain\", \"#BCN\", \"#BARCELONA\",\n",
    "                        \"#Espana\", \"#BarcelonaSpain\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create documents\n",
    "documents_hashtag_pooling = hashtags_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first objective of the research is to analyze the distribution of topics over the districts. For this purpose, district pooling is used to create the documents that will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from each district (district pooling) and treat them as one document respectively\n",
    "district_combined = english_tweets[[\"text_clean\",\"idBarrio\"]].groupby(\"idBarrio\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_pooling = district_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second objective is to look at the dynamic topic development over time. For this purpose, the dataset is divided according to time and documents are created on this basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataframe according to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort index\n",
    "sorted_tweets = english_tweets.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-11 13:46:35\n",
      "2017-12-04 21:20:25\n"
     ]
    }
   ],
   "source": [
    "# check first and last date\n",
    "print(sorted_tweets.index[0]) # June 2017\n",
    "print(sorted_tweets.index[-1]) # December 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column that contains the month of the tweets\n",
    "sorted_tweets['month'] = sorted_tweets.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # split dataframe according to month\n",
    "# june = sorted_tweets.loc['2017-06-01':'2017-06-30']\n",
    "# july = sorted_tweets.loc['2017-07-01':'2017-07-31']\n",
    "# august = sorted_tweets.loc['2017-08-01':'2017-08-31']\n",
    "# september = sorted_tweets.loc['2017-09-01':'2017-09-30']\n",
    "# october = sorted_tweets.loc['2017-10-01':'2017-10-31']\n",
    "# november = sorted_tweets.loc['2017-11-01':'2017-11-30']\n",
    "# december = sorted_tweets.loc['2017-12-01':'2017-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from each month and treat them as one document respectively\n",
    "months_combined = english_tweets[[\"text_clean\",\"month\"]].groupby(\"month\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_month_pooling = months_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many documents\n",
    "len(documents_month_pooling) # should be 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from one district and one month and treat them as one document respectively\n",
    "districts_per_month_combined = sorted_tweets[[\"text_clean\",\"month\",\"idBarrio\"]].groupby([\"month\",\"idBarrio\"])[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_per_month_pooling = districts_per_month_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many documents\n",
    "len(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month  idBarrio\n",
       "6      1           Our last night in #Barcelona having dinner at ...\n",
       "       2           Boy don't hurt your brain  Gothic Quarter Barc...\n",
       "       3           Hearbeat  #boyfriend #bf #bcn #love #spain #pa...\n",
       "       4           Just posted a photo  C/mònec 14 Barcelona Ramo...\n",
       "       5           Un salto a la realidad #sweetdreams #bcn #summ...\n",
       "       6           I was moved  inspired  My life will never be t...\n",
       "       7           Another for the #Pokedex  PokemonGo Barcelona ...\n",
       "       8           Special Jazz concert Sat 17th June 17  22:30  ...\n",
       "       9           #suit #knife #scene #killers  Gran Via de les ...\n",
       "       10          Just posted a photo  i  Barcelona mellowsheng ...\n",
       "       11          Fireworks I II III by Joan Miró  Fundació Joan...\n",
       "       12          CARAVANA #casualcomposition #waytosee #francba...\n",
       "       14          We had a great time last week for the LouLou r...\n",
       "       15          We are ready for thisOff Week 2017 is here! #m...\n",
       "       16          #recording #studio #guestvocals #boqueroneando...\n",
       "       17          Just posted a photo  Placa Del Sol #metro #str...\n",
       "       18          Another great day shooting a short movie  #fee...\n",
       "       19          Morning concepts #everisdesign #design #ux #ev...\n",
       "       20          JoelJoanJuveView from the Endangered Alphabets...\n",
       "       21          I just finished running 576 km in 44m:12s with...\n",
       "       23          Bring your swimming costume!\\nThursday 15th (1...\n",
       "       24          R of #coffee #milk #art #office #friends #iger...\n",
       "       25          Lots of walks some hikes sightseeing Sangria f...\n",
       "       26          Home is where the heart is\\n#Barcelona #barcel...\n",
       "       27          nothing like 2 tiempos awesome bike #bultaco #...\n",
       "       28            goonie4ever_ lhe vist tota Divertimento brutal\n",
       "       30          I like the shadow and light in this #iphone pi...\n",
       "       31          This week I'll be sharing my process for brody...\n",
       "       32          Just posted a photo  La Innovation Kitchen de ...\n",
       "       35          RedChilliesEnt: BollySongs_BCN Here's a specia...\n",
       "                                         ...                        \n",
       "11     46          Getting therapy ( Betty Ford's in Barcelona w/...\n",
       "       48          Plan your next #vacation experience to the bea...\n",
       "       49          Gaudis Le Pedrera  Casa Mila!\\n#beautiful #ge...\n",
       "       50          Really inspirational speech by #Montreauxjazzf...\n",
       "       51          Just posted a photo  L3 I just finished runnin...\n",
       "       52          79% of the most successfull #companies do #Inv...\n",
       "       53          Wish I had a day like this today but instead I...\n",
       "       54          #bird Arte urbano Barcelona #digerible #arteur...\n",
       "       55          On Thursdays cdlcbarcelona  #cdlcbarcelona #th...\n",
       "       56          halloWEAVE  #justtrynahitforhalloween  Shôko B...\n",
       "       57          Republic day 9\\n\\n#freecatalanpoliticalprisone...\n",
       "       59          Sharks  everywhere!  L'Aquàrium de Barcelona #...\n",
       "       60          Reencuentros \\n\\n\\n\\n\\n\\n\\n\\n#party #love #mus...\n",
       "       61          Idonuts #watercolor #winsorandnewton #contempo...\n",
       "       62          Happiness Tribute to legend M_Granollers ! #fu...\n",
       "       63          PS I did have red hair and red lipstick but I'...\n",
       "       64          happy birthday CasioBaird I hope youre having...\n",
       "       65          What a luxurious feeling to be on the broadest...\n",
       "       66          The view from Arenas \\n\\n#JLQTravels  #JanEsc...\n",
       "       67          Sunday evening celebrating I have completed my...\n",
       "       68          Drinking a Heretic (2017) by Cervesa Artesana ...\n",
       "       69          Kibukeando jt51rulo esteramigo14 #japanese #ba...\n",
       "       70          Continuing our #Barcelona culture vulture #wal...\n",
       "       71          Life is short break the rules forgive quickly ...\n",
       "       72             Sun and waves  day! en Platja de la Mar Bella \n",
       "       73          Place of love +Alice à Rambla de CATALUNYA So ...\n",
       "12     40          Gaudi awesomeness  Basílica de la Sagrada Famí...\n",
       "       43          Quick one to start #saturday  Inspired this ti...\n",
       "       49          Drinking a Leyenda by CervezaDougalls at cocov...\n",
       "       51          Brama (at CrosmasLounge in Barcelona w/ terraf...\n",
       "Name: text_clean, Length: 328, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataframe\n",
    "districts_per_month_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training and Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('documents.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents:\n",
    "        f.write(item)\n",
    "with io.open('documents_user_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_user_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_hashtag_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_hashtag_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_district_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_month_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_district_per_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_per_month_pooling:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_no_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents]\n",
    "\n",
    "texts_user_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_user_pooling]\n",
    "\n",
    "texts_hashtag_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing of Training Documents after Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove numbers, but not words that contain numbers.\n",
    "texts_no_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that are only one character.\n",
    "texts_no_pooling = [[token for token in doc if len(token) > 1] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if len(token) > 1] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if len(token) > 1] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize all words in all documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts_no_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part! computing bigrams did not improve models but made them worse!\n",
    "\n",
    "# # compute bigrams\n",
    "# # add bigrams and trigrams to docs (only ones that appear 5 times or more)\n",
    "# bigram = Phrases(texts_no_pooling, min_count=10)\n",
    "# for idx in range(len(texts_no_pooling)):\n",
    "#     for token in bigram[texts_no_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_no_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_user_pooling, min_count=10)\n",
    "# for idx in range(len(texts_user_pooling)):\n",
    "#     for token in bigram[texts_user_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_user_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_hashtag_pooling, min_count=10)\n",
    "# for idx in range(len(texts_hashtag_pooling)):\n",
    "#     for token in bigram[texts_hashtag_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_hashtag_pooling[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Tokenized Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenized_documents_no_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_no_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_user_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_user_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_hashtag_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_hashtag_pooling, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine and Vectorize Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to refine and vectorize corpus \n",
    "# (remove stopwords, very frequent and very infrequent words etc.)\n",
    "\n",
    "# define stopwords\n",
    "stpwords = 'for a of the and to in at by spain barcelona #barcelona #spain de la del en las \"barcelona #bcn'.split()\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, \n",
    "                additional_stopwords=set(stpwords),\n",
    "                no_below=2, no_above=0.5,\n",
    "                dictionary_name='tourism.dict', corpus_name='tourism.mm'):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    dictionary.compactify()\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    dictionary.save(dictionary_name)  # store the dictionary, for future reference\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize(corpus_name, corpus)  # store to disk, for later use\n",
    "    \n",
    "    return (corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:34,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:34,236 : INFO : built Dictionary(16741 unique tokens: ['#olgodbarcelona', '#triathlontraining', '#ølgod', 'beer', 'brew']...) from 7633 documents (total 79537 corpus positions)\n",
      "2018-09-20 18:52:34,325 : INFO : discarding 11562 tokens: [('#olgodbarcelona', 1), ('#triathlontraining', 1), ('quickie', 1), ('#hadthemostmusselsinmylife', 1), ('#tripoftheyear', 1), ('feast!', 1), ('freshest', 1), ('hail', 1), ('2h:27m:58s', 1), ('mallan', 1)]...\n",
      "2018-09-20 18:52:34,326 : INFO : keeping 5035 tokens which were in no less than 2 and no more than 3816 (=50.0%) documents\n",
      "2018-09-20 18:52:34,340 : INFO : resulting dictionary: Dictionary(5035 unique tokens: ['#ølgod', 'beer', 'brew', 'offer', 'pub']...)\n",
      "2018-09-20 18:52:34,345 : INFO : saving Dictionary object under tourism_no_pooling.dict, separately None\n",
      "2018-09-20 18:52:34,353 : INFO : saved tourism_no_pooling.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:34,622 : INFO : storing corpus in Matrix Market format to tourism_no_pooling.mm\n",
      "2018-09-20 18:52:34,626 : INFO : saving sparse matrix to tourism_no_pooling.mm\n",
      "2018-09-20 18:52:34,628 : INFO : PROGRESS: saving document #0\n",
      "2018-09-20 18:52:34,657 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-20 18:52:34,690 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-20 18:52:34,728 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:34,761 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:34,794 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-20 18:52:34,822 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-20 18:52:34,858 : INFO : PROGRESS: saving document #7000\n",
      "2018-09-20 18:52:34,883 : INFO : saved 7633x5035 matrix, density=0.108% (41550/38432155)\n",
      "2018-09-20 18:52:34,886 : INFO : saving MmCorpus index to tourism_no_pooling.mm.index\n",
      "2018-09-20 18:52:34,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:35,136 : INFO : built Dictionary(16741 unique tokens: ['#olgodbarcelona', '#triathlontraining', '#ølgod', 'beer', 'brew']...) from 7633 documents (total 79537 corpus positions)\n",
      "2018-09-20 18:52:35,211 : INFO : discarding 11562 tokens: [('#olgodbarcelona', 1), ('#triathlontraining', 1), ('quickie', 1), ('#hadthemostmusselsinmylife', 1), ('#tripoftheyear', 1), ('feast!', 1), ('freshest', 1), ('hail', 1), ('2h:27m:58s', 1), ('mallan', 1)]...\n",
      "2018-09-20 18:52:35,213 : INFO : keeping 5035 tokens which were in no less than 2 and no more than 3816 (=50.0%) documents\n",
      "2018-09-20 18:52:35,225 : INFO : resulting dictionary: Dictionary(5035 unique tokens: ['#ølgod', 'beer', 'brew', 'offer', 'pub']...)\n",
      "2018-09-20 18:52:35,230 : INFO : saving Dictionary object under tourism_no_pooling.dict, separately None\n",
      "2018-09-20 18:52:35,239 : INFO : saved tourism_no_pooling.dict\n",
      "2018-09-20 18:52:35,397 : INFO : storing corpus in Matrix Market format to tourism_no_pooling.mm\n",
      "2018-09-20 18:52:35,400 : INFO : saving sparse matrix to tourism_no_pooling.mm\n",
      "2018-09-20 18:52:35,403 : INFO : PROGRESS: saving document #0\n",
      "2018-09-20 18:52:35,425 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-20 18:52:35,458 : INFO : PROGRESS: saving document #2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:35,494 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:35,526 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:35,560 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-20 18:52:35,592 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-20 18:52:35,629 : INFO : PROGRESS: saving document #7000\n",
      "2018-09-20 18:52:35,657 : INFO : saved 7633x5035 matrix, density=0.108% (41550/38432155)\n",
      "2018-09-20 18:52:35,659 : INFO : saving MmCorpus index to tourism_no_pooling.mm.index\n",
      "2018-09-20 18:52:35,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:35,950 : INFO : built Dictionary(16761 unique tokens: ['castellano', 'catalangov:', 'cataluña', 'consecuencia', 'country']...) from 4424 documents (total 79383 corpus positions)\n",
      "2018-09-20 18:52:36,011 : INFO : discarding 12432 tokens: [('castellano', 1), ('catalangov:', 1), ('consecuencia', 1), ('exclusion', 1), ('instil', 1), ('linguistica', 1), ('mala', 1), ('politica', 1), ('practica', 1), ('shameful', 1)]...\n",
      "2018-09-20 18:52:36,012 : INFO : keeping 4185 tokens which were in no less than 2 and no more than 2212 (=50.0%) documents\n",
      "2018-09-20 18:52:36,023 : INFO : resulting dictionary: Dictionary(4185 unique tokens: ['cataluña', 'country', 'e', 'fear', 'police']...)\n",
      "2018-09-20 18:52:36,027 : INFO : saving Dictionary object under tourism_user_pooling.dict, separately None\n",
      "2018-09-20 18:52:36,034 : INFO : saved tourism_user_pooling.dict\n",
      "2018-09-20 18:52:36,150 : INFO : storing corpus in Matrix Market format to tourism_user_pooling.mm\n",
      "2018-09-20 18:52:36,152 : INFO : saving sparse matrix to tourism_user_pooling.mm\n",
      "2018-09-20 18:52:36,154 : INFO : PROGRESS: saving document #0\n",
      "2018-09-20 18:52:36,212 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-20 18:52:36,241 : INFO : PROGRESS: saving document #2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:36,271 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:36,305 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:36,315 : INFO : saved 4424x4185 matrix, density=0.182% (33668/18514440)\n",
      "2018-09-20 18:52:36,319 : INFO : saving MmCorpus index to tourism_user_pooling.mm.index\n",
      "2018-09-20 18:52:36,325 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-09-20 18:52:36,519 : INFO : built Dictionary(16761 unique tokens: ['castellano', 'catalangov:', 'cataluña', 'consecuencia', 'country']...) from 4424 documents (total 79383 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:36,596 : INFO : discarding 12432 tokens: [('castellano', 1), ('catalangov:', 1), ('consecuencia', 1), ('exclusion', 1), ('instil', 1), ('linguistica', 1), ('mala', 1), ('politica', 1), ('practica', 1), ('shameful', 1)]...\n",
      "2018-09-20 18:52:36,599 : INFO : keeping 4185 tokens which were in no less than 2 and no more than 2212 (=50.0%) documents\n",
      "2018-09-20 18:52:36,615 : INFO : resulting dictionary: Dictionary(4185 unique tokens: ['cataluña', 'country', 'e', 'fear', 'police']...)\n",
      "2018-09-20 18:52:36,625 : INFO : saving Dictionary object under tourism_user_pooling.dict, separately None\n",
      "2018-09-20 18:52:36,630 : INFO : saved tourism_user_pooling.dict\n",
      "2018-09-20 18:52:36,743 : INFO : storing corpus in Matrix Market format to tourism_user_pooling.mm\n",
      "2018-09-20 18:52:36,746 : INFO : saving sparse matrix to tourism_user_pooling.mm\n",
      "2018-09-20 18:52:36,750 : INFO : PROGRESS: saving document #0\n",
      "2018-09-20 18:52:36,803 : INFO : PROGRESS: saving document #1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:36,856 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-20 18:52:36,884 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:36,921 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:36,934 : INFO : saved 4424x4185 matrix, density=0.182% (33668/18514440)\n",
      "2018-09-20 18:52:36,937 : INFO : saving MmCorpus index to tourism_user_pooling.mm.index\n",
      "2018-09-20 18:52:36,954 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:37,311 : INFO : built Dictionary(11970 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...) from 6198 documents (total 148488 corpus positions)\n",
      "2018-09-20 18:52:37,358 : INFO : discarding 1931 tokens: [('(out', 1), ('10k++)', 1), (\"advisor's\", 1), ('carot', 1), ('celebration', 1), ('forgotten', 1), ('garden!', 1), ('napa', 1), ('oriental', 1), ('skewer', 1)]...\n",
      "2018-09-20 18:52:37,360 : INFO : keeping 9904 tokens which were in no less than 2 and no more than 3099 (=50.0%) documents\n",
      "2018-09-20 18:52:37,379 : INFO : resulting dictionary: Dictionary(9904 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...)\n",
      "2018-09-20 18:52:37,387 : INFO : saving Dictionary object under tourism_hashtag_pooling.dict, separately None\n",
      "2018-09-20 18:52:37,397 : INFO : saved tourism_hashtag_pooling.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:37,615 : INFO : storing corpus in Matrix Market format to tourism_hashtag_pooling.mm\n",
      "2018-09-20 18:52:37,619 : INFO : saving sparse matrix to tourism_hashtag_pooling.mm\n",
      "2018-09-20 18:52:37,621 : INFO : PROGRESS: saving document #0\n",
      "2018-09-20 18:52:37,675 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-20 18:52:37,734 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-20 18:52:37,780 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:37,836 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:37,912 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-20 18:52:37,979 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-20 18:52:37,991 : INFO : saved 6198x9904 matrix, density=0.138% (84677/61384992)\n",
      "2018-09-20 18:52:37,993 : INFO : saving MmCorpus index to tourism_hashtag_pooling.mm.index\n",
      "2018-09-20 18:52:37,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:38,396 : INFO : built Dictionary(11970 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...) from 6198 documents (total 148488 corpus positions)\n",
      "2018-09-20 18:52:38,433 : INFO : discarding 1931 tokens: [('(out', 1), ('10k++)', 1), (\"advisor's\", 1), ('carot', 1), ('celebration', 1), ('forgotten', 1), ('garden!', 1), ('napa', 1), ('oriental', 1), ('skewer', 1)]...\n",
      "2018-09-20 18:52:38,438 : INFO : keeping 9904 tokens which were in no less than 2 and no more than 3099 (=50.0%) documents\n",
      "2018-09-20 18:52:38,448 : INFO : resulting dictionary: Dictionary(9904 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...)\n",
      "2018-09-20 18:52:38,455 : INFO : saving Dictionary object under tourism_hashtag_pooling.dict, separately None\n",
      "2018-09-20 18:52:38,464 : INFO : saved tourism_hashtag_pooling.dict\n",
      "2018-09-20 18:52:38,648 : INFO : storing corpus in Matrix Market format to tourism_hashtag_pooling.mm\n",
      "2018-09-20 18:52:38,651 : INFO : saving sparse matrix to tourism_hashtag_pooling.mm\n",
      "2018-09-20 18:52:38,653 : INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-20 18:52:38,693 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-20 18:52:38,756 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-20 18:52:38,800 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-20 18:52:38,835 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-20 18:52:38,872 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-20 18:52:38,914 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-20 18:52:38,922 : INFO : saved 6198x9904 matrix, density=0.138% (84677/61384992)\n",
      "2018-09-20 18:52:38,924 : INFO : saving MmCorpus index to tourism_hashtag_pooling.mm.index\n"
     ]
    }
   ],
   "source": [
    "# run function to vectorize corpora\n",
    "corpus_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                corpus_name=\"tourism_no_pooling.mm\")[0]\n",
    "dictionary_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                    dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_no_pooling.mm\")[1]\n",
    "\n",
    "corpus_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[0]\n",
    "dictionary_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[1]\n",
    "\n",
    "corpus_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[0]\n",
    "dictionary_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Function to Preprocess Test Documents (Before Testing Them with LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function has to include all the same steps that were applied to the training documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def preprocess(docs):\n",
    "    ''' Conduct all preprocessing steps that are conducted to train the LDA model'''\n",
    "    \n",
    "    # tokenize documents\n",
    "    tokenized = [[word for word in document.lower().split()]\n",
    "          for document in docs]\n",
    "    \n",
    "    # remove words that are only one character\n",
    "    tokenized = [[token for token in doc if len(token) > 1] for doc in tokenized]\n",
    "    \n",
    "    # lemmatize all words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in tokenized]\n",
    "    \n",
    "    # define stopwords\n",
    "    stpwords = 'for a of the and to in at by spain barcelona #barcelona #spain de la del en las \"barcelona #bcn'.split()\n",
    "    \n",
    "    # get stopwords from nltk\n",
    "    def nltk_stopwords():\n",
    "        return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # combine stopwords\n",
    "    stopwords = nltk_stopwords().union(stpwords)\n",
    "    \n",
    "    # remove stopwords\n",
    "    preprocessed = [[token for token in document if token not in stopwords] for document in lemmatized]\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply function to test documents\n",
    "texts_district_pooling = preprocess(documents_district_pooling)\n",
    "texts_month_pooling = preprocess(documents_month_pooling)\n",
    "texts_district_per_month_pooling = preprocess(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenized_documents_district_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_month_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_district_per_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_per_month_pooling, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # map tokens to ids\n",
    "# print(dictionary_no_pooling.token2id)\n",
    "# print(dictionary_user_pooling.token2id)\n",
    "# print(dictionary_hashtag_pooling.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # convert new document to vector \n",
    "# new_doc = \"Sagrada Familia is amazing\"\n",
    "# new_vec_no_pooling = dictionary_no_pooling.doc2bow(new_doc.lower().split())\n",
    "# print(new_vec_no_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part! not needed for dataset!\n",
    "\n",
    "# # corpus streaming: one document at a time\n",
    "# class MyCorpus(object):\n",
    "#     def __iter__(self):\n",
    "#         for line in open(\"corpus_no_pooling.txt\"):\n",
    "#             # assume there's one document per line, tokens separated by whitespace\n",
    "#             yield dictionary.doc2bow(line.lower().split())\n",
    "#             \n",
    "# corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "# print(corpus_memory_friendly)\n",
    "# \n",
    "# for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "#     print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
