{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/tut1.html\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebastian Birk\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sebastian\n",
      "[nltk_data]     Birk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sebastian\n",
      "[nltk_data]     Birk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords and lemmatizer from nltk package\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log events\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data with timestamp as index\n",
    "tweets = pd.read_csv(\"tweets.csv\", encoding=\"latin1\", parse_dates=True, \n",
    "                     index_col=\"created\", usecols=range(1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>retweetCountOutlier</th>\n",
       "      <th>tweetcount</th>\n",
       "      <th>movement</th>\n",
       "      <th>language3</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weeknumber</th>\n",
       "      <th>month</th>\n",
       "      <th>idBarrio_xy</th>\n",
       "      <th>idBarrio</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:44:07</th>\n",
       "      <td>I'm at El Raval in Barcelona https://t.co/bSGA...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://foursquare.com\" rel=\"nofollow\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.168964</td>\n",
       "      <td>41.380936</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>48</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u03883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 19:48:53</th>\n",
       "      <td>&lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BC&gt;&lt;ed&gt;&lt;U+00B6&gt;&lt;U+0098&gt; @ O't...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.168180</td>\n",
       "      <td>41.381031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u02046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 21:58:48</th>\n",
       "      <td>Aquesta setmana publiquem una nova escapada al...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168721</td>\n",
       "      <td>41.380217</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CATALAN</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u03884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 11:15:10</th>\n",
       "      <td>I'm at El Raval in Barcelona https://t.co/xz2A...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://foursquare.com\" rel=\"nofollow\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.168964</td>\n",
       "      <td>41.380936</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Monday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u00881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 10:08:51</th>\n",
       "      <td>Hablan catalán y es importante destacar que el...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.168180</td>\n",
       "      <td>41.381031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SPANISH</td>\n",
       "      <td>Monday</td>\n",
       "      <td>47</td>\n",
       "      <td>November</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>u02047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "created                                                                  \n",
       "2017-11-28 22:44:07  I'm at El Raval in Barcelona https://t.co/bSGA...   \n",
       "2017-11-22 19:48:53  <ed><U+00A0><U+00BC><ed><U+00B6><U+0098> @ O't...   \n",
       "2017-11-21 21:58:48  Aquesta setmana publiquem una nova escapada al...   \n",
       "2017-11-20 11:15:10  I'm at El Raval in Barcelona https://t.co/xz2A...   \n",
       "2017-11-20 10:08:51  Hablan catalán y es importante destacar que el...   \n",
       "\n",
       "                     favoriteCount replyToSN  truncated  replyToSID  \\\n",
       "created                                                               \n",
       "2017-11-28 22:44:07              0       NaN      False         NaN   \n",
       "2017-11-22 19:48:53              0       NaN      False         NaN   \n",
       "2017-11-21 21:58:48              1       NaN      False         NaN   \n",
       "2017-11-20 11:15:10              0       NaN      False         NaN   \n",
       "2017-11-20 10:08:51              0       NaN      False         NaN   \n",
       "\n",
       "                     replyToUID  \\\n",
       "created                           \n",
       "2017-11-28 22:44:07         NaN   \n",
       "2017-11-22 19:48:53         NaN   \n",
       "2017-11-21 21:58:48         NaN   \n",
       "2017-11-20 11:15:10         NaN   \n",
       "2017-11-20 10:08:51         NaN   \n",
       "\n",
       "                                                          statusSource  \\\n",
       "created                                                                  \n",
       "2017-11-28 22:44:07  <a href=\"http://foursquare.com\" rel=\"nofollow\"...   \n",
       "2017-11-22 19:48:53  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-21 21:58:48  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2017-11-20 11:15:10  <a href=\"http://foursquare.com\" rel=\"nofollow\"...   \n",
       "2017-11-20 10:08:51  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "\n",
       "                     retweetCount  longitude   latitude   ...    \\\n",
       "created                                                   ...     \n",
       "2017-11-28 22:44:07             0   2.168964  41.380936   ...     \n",
       "2017-11-22 19:48:53             0   2.168180  41.381031   ...     \n",
       "2017-11-21 21:58:48             1   2.168721  41.380217   ...     \n",
       "2017-11-20 11:15:10             0   2.168964  41.380936   ...     \n",
       "2017-11-20 10:08:51             0   2.168180  41.381031   ...     \n",
       "\n",
       "                     retweetCountOutlier tweetcount  movement language3  \\\n",
       "created                                                                   \n",
       "2017-11-28 22:44:07                    0          1       1.0     OTHER   \n",
       "2017-11-22 19:48:53                    0          2       1.0     OTHER   \n",
       "2017-11-21 21:58:48                    0          1       1.0   CATALAN   \n",
       "2017-11-20 11:15:10                    0          4       1.0     OTHER   \n",
       "2017-11-20 10:08:51                    0          2       1.0   SPANISH   \n",
       "\n",
       "                     dayofweek  weeknumber     month  idBarrio_xy  idBarrio  \\\n",
       "created                                                                       \n",
       "2017-11-28 22:44:07    Tuesday          48  November            1        55   \n",
       "2017-11-22 19:48:53  Wednesday          47  November            1        55   \n",
       "2017-11-21 21:58:48    Tuesday          47  November            1        55   \n",
       "2017-11-20 11:15:10     Monday          47  November            1        55   \n",
       "2017-11-20 10:08:51     Monday          47  November            1        55   \n",
       "\n",
       "                       user  \n",
       "created                      \n",
       "2017-11-28 22:44:07  u03883  \n",
       "2017-11-22 19:48:53  u02046  \n",
       "2017-11-21 21:58:48  u03884  \n",
       "2017-11-20 11:15:10  u00881  \n",
       "2017-11-20 10:08:51  u02047  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataframe\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 23778 entries, 2017-11-28 22:44:07 to 2017-06-11 15:55:42\n",
      "Data columns (total 26 columns):\n",
      "text                    23778 non-null object\n",
      "favoriteCount           23778 non-null int64\n",
      "replyToSN               821 non-null object\n",
      "truncated               23778 non-null bool\n",
      "replyToSID              574 non-null float64\n",
      "replyToUID              821 non-null float64\n",
      "statusSource            23778 non-null object\n",
      "retweetCount            23778 non-null int64\n",
      "longitude               23778 non-null float64\n",
      "latitude                23778 non-null float64\n",
      "id_seccion              23778 non-null int64\n",
      "horaPeticion            23778 non-null object\n",
      "id_distrito             23778 non-null int64\n",
      "grupoHora               23778 non-null object\n",
      "id_seccion_xy           23778 non-null int64\n",
      "favoriteCountOutlier    23778 non-null int64\n",
      "retweetCountOutlier     23778 non-null int64\n",
      "tweetcount              23778 non-null int64\n",
      "movement                23778 non-null float64\n",
      "language3               23778 non-null object\n",
      "dayofweek               23778 non-null object\n",
      "weeknumber              23778 non-null int64\n",
      "month                   23778 non-null object\n",
      "idBarrio_xy             23778 non-null int64\n",
      "idBarrio                23778 non-null int64\n",
      "user                    23778 non-null object\n",
      "dtypes: bool(1), float64(5), int64(11), object(9)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# display dataframe info\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>id_seccion</th>\n",
       "      <th>id_distrito</th>\n",
       "      <th>id_seccion_xy</th>\n",
       "      <th>favoriteCountOutlier</th>\n",
       "      <th>retweetCountOutlier</th>\n",
       "      <th>tweetcount</th>\n",
       "      <th>movement</th>\n",
       "      <th>weeknumber</th>\n",
       "      <th>idBarrio_xy</th>\n",
       "      <th>idBarrio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23778.000000</td>\n",
       "      <td>5.740000e+02</td>\n",
       "      <td>8.210000e+02</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>2.377800e+04</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>2.377800e+04</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "      <td>23778.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.200774</td>\n",
       "      <td>9.015351e+17</td>\n",
       "      <td>4.078693e+16</td>\n",
       "      <td>0.285222</td>\n",
       "      <td>2.171064</td>\n",
       "      <td>41.395325</td>\n",
       "      <td>8.019040e+08</td>\n",
       "      <td>801903.985953</td>\n",
       "      <td>7.999476e+08</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>17.686349</td>\n",
       "      <td>0.690218</td>\n",
       "      <td>34.522962</td>\n",
       "      <td>18.136008</td>\n",
       "      <td>21.180503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.403217</td>\n",
       "      <td>5.293076e+16</td>\n",
       "      <td>1.808261e+17</td>\n",
       "      <td>4.610914</td>\n",
       "      <td>0.021190</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>3.028198e+03</td>\n",
       "      <td>3.023254</td>\n",
       "      <td>3.955736e+07</td>\n",
       "      <td>0.025932</td>\n",
       "      <td>0.011232</td>\n",
       "      <td>62.044925</td>\n",
       "      <td>0.326073</td>\n",
       "      <td>7.046783</td>\n",
       "      <td>21.415965</td>\n",
       "      <td>22.107374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.229677e+17</td>\n",
       "      <td>7.802900e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.059243</td>\n",
       "      <td>41.332580</td>\n",
       "      <td>8.019010e+08</td>\n",
       "      <td>801901.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.862349e+17</td>\n",
       "      <td>1.195479e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.159720</td>\n",
       "      <td>41.382780</td>\n",
       "      <td>8.019020e+08</td>\n",
       "      <td>801902.000000</td>\n",
       "      <td>8.019020e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.079774e+17</td>\n",
       "      <td>3.537928e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.174778</td>\n",
       "      <td>41.395250</td>\n",
       "      <td>8.019021e+08</td>\n",
       "      <td>801902.000000</td>\n",
       "      <td>8.019021e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.232741e+17</td>\n",
       "      <td>1.028215e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.176944</td>\n",
       "      <td>41.404080</td>\n",
       "      <td>8.019060e+08</td>\n",
       "      <td>801906.000000</td>\n",
       "      <td>8.019050e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2449.000000</td>\n",
       "      <td>9.354775e+17</td>\n",
       "      <td>9.290872e+17</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>2.226620</td>\n",
       "      <td>41.465590</td>\n",
       "      <td>8.019102e+08</td>\n",
       "      <td>801910.000000</td>\n",
       "      <td>8.019102e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       favoriteCount    replyToSID    replyToUID  retweetCount     longitude  \\\n",
       "count   23778.000000  5.740000e+02  8.210000e+02  23778.000000  23778.000000   \n",
       "mean        1.200774  9.015351e+17  4.078693e+16      0.285222      2.171064   \n",
       "std        20.403217  5.293076e+16  1.808261e+17      4.610914      0.021190   \n",
       "min         0.000000  1.229677e+17  7.802900e+05      0.000000      2.059243   \n",
       "25%         0.000000  8.862349e+17  1.195479e+08      0.000000      2.159720   \n",
       "50%         0.000000  9.079774e+17  3.537928e+08      0.000000      2.174778   \n",
       "75%         1.000000  9.232741e+17  1.028215e+09      0.000000      2.176944   \n",
       "max      2449.000000  9.354775e+17  9.290872e+17    567.000000      2.226620   \n",
       "\n",
       "           latitude    id_seccion    id_distrito  id_seccion_xy  \\\n",
       "count  23778.000000  2.377800e+04   23778.000000   2.377800e+04   \n",
       "mean      41.395325  8.019040e+08  801903.985953   7.999476e+08   \n",
       "std        0.014936  3.028198e+03       3.023254   3.955736e+07   \n",
       "min       41.332580  8.019010e+08  801901.000000   0.000000e+00   \n",
       "25%       41.382780  8.019020e+08  801902.000000   8.019020e+08   \n",
       "50%       41.395250  8.019021e+08  801902.000000   8.019021e+08   \n",
       "75%       41.404080  8.019060e+08  801906.000000   8.019050e+08   \n",
       "max       41.465590  8.019102e+08  801910.000000   8.019102e+08   \n",
       "\n",
       "       favoriteCountOutlier  retweetCountOutlier    tweetcount      movement  \\\n",
       "count          23778.000000         23778.000000  23778.000000  23778.000000   \n",
       "mean               0.000673             0.000126     17.686349      0.690218   \n",
       "std                0.025932             0.011232     62.044925      0.326073   \n",
       "min                0.000000             0.000000      1.000000      0.100000   \n",
       "25%                0.000000             0.000000      1.000000      0.384615   \n",
       "50%                0.000000             0.000000      3.000000      0.750000   \n",
       "75%                0.000000             0.000000      8.000000      1.000000   \n",
       "max                1.000000             1.000000    440.000000      1.000000   \n",
       "\n",
       "         weeknumber   idBarrio_xy      idBarrio  \n",
       "count  23778.000000  23778.000000  23778.000000  \n",
       "mean      34.522962     18.136008     21.180503  \n",
       "std        7.046783     21.415965     22.107374  \n",
       "min       23.000000      0.000000      1.000000  \n",
       "25%       28.000000      6.000000      6.000000  \n",
       "50%       34.000000      7.000000      9.000000  \n",
       "75%       40.000000     25.000000     31.000000  \n",
       "max       49.000000     73.000000     73.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe dataframe \n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide dataset according to language: extract english language\n",
    "english_tweets = tweets[tweets[\"language3\"] == \"ENGLISH\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess twitter text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove links\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text\"].str.replace(r\"http\\S+\", \"\")\n",
    "# remove emoticons\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"<.*>\", \"\")\n",
    "# remove @s\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"@\", \"\")\n",
    "# remove punctuation and special characters\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"&amp\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\.\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\,\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\;\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\-\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "english_tweets.columns\n",
    "cols = ['text', 'text_clean', 'favoriteCount', 'replyToSN', 'truncated', 'replyToSID',\n",
    "       'replyToUID', 'statusSource', 'retweetCount', 'longitude', 'latitude',\n",
    "       'id_seccion', 'horaPeticion', 'id_distrito', 'grupoHora',\n",
    "       'id_seccion_xy', 'favoriteCountOutlier', 'retweetCountOutlier',\n",
    "       'tweetcount', 'movement', 'language3', 'dayofweek', 'weeknumber',\n",
    "       'month', 'idBarrio_xy', 'idBarrio', 'user']\n",
    "\n",
    "english_tweets = english_tweets[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to train the topic models, 3 different pooling methods for the creation of documents are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 1 (No Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat every tweet as a document (no pooling)\n",
    "documents = english_tweets[\"text_clean\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 2 (User Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat all tweets by one user as a document (user pooling)\n",
    "user_combined = english_tweets[[\"text_clean\",\"user\"]].groupby(\"user\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_user_pooling = user_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Training Documents Option 3 (Hashtag Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treat all tweets with same hashtags as a document (hashtag pooling)\n",
    "\n",
    "# find all hashtags\n",
    "english_tweets[\"hashtags\"] = english_tweets[\"text_clean\"].str.findall(r'#.*?(?=\\s|$)')\n",
    "\n",
    "# separate hashtags in columns\n",
    "hashtags_tweets = pd.DataFrame(english_tweets[\"hashtags\"].tolist(),\n",
    "                               columns=[\"hashtag1\", \"hashtag2\", \"hashtag3\", \"hashtag4\",\n",
    "                                        \"hashtag5\", \"hashtag6\", \"hashtag7\", \"hashtag8\",\n",
    "                                        \"hashtag9\", \"hashtag10\", \"hashtag11\", \"hashtag12\",\n",
    "                                        \"hashtag13\"])\n",
    "\n",
    "# join hashtags with tweet text\n",
    "hashtags_tweets.index = english_tweets.index\n",
    "hashtags_tweets = english_tweets.join(hashtags_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create one dataframe with text for each hashtag column and save them in a dictionary\n",
    "dict = {}\n",
    "for index, item in enumerate([\"hash1\", \"hash2\", \"hash3\", \"hash4\", \"hash5\",\n",
    "                              \"hash6\", \"hash7\", \"hash8\", \"hash9\", \"hash10\",\n",
    "                              \"hash11\", \"hash12\", \"hash13\"]):\n",
    "    dict[item] = hashtags_tweets[[\"hashtag\" + str(index + 1), \"text_clean\"]].copy()\n",
    "    dict[item].columns = [\"hashtag\", \"text\"]\n",
    "    dict[item].dropna(inplace=True)\n",
    "\n",
    "# concatenate all dataframes to one dataframe (the result is a dataframe\n",
    "# where there is text for each hashtag found)\n",
    "hashtags = pd.DataFrame()\n",
    "for item in dict:\n",
    "    hashtags = pd.concat([hashtags, dict[item]])\n",
    "\n",
    "# combine text for each hashtag\n",
    "hashtags_combined = hashtags.groupby(\"hashtag\")[\"text\"].apply(lambda x: \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove some generic hashtags that cover a lot of different topics\n",
    "hashtags_combined.drop([\"#Barcelona\", \"#Catalunya\", \"#Spain\", \"#BCN\", \"#BARCELONA\",\n",
    "                        \"#Espana\", \"#BarcelonaSpain\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create documents\n",
    "documents_hashtag_pooling = hashtags_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first objective of the research is to analyze the distribution of topics over the districts. For this purpose, district pooling is used to create the documents that will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from each district (district pooling) and treat them as one document respectively\n",
    "district_combined = english_tweets[[\"text_clean\",\"id_distrito\"]].groupby(\"id_distrito\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_pooling = district_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_distrito\n",
       "801901    #cure ( Betty Ford's in Barcelona) Sunday fund...\n",
       "801902    Barcelona Plaza de España #usk #urbansketchers...\n",
       "801903    Get up work [enjoy] sleep repeat #workroutine ...\n",
       "801904    Last Tuesday hyde_club honored to play leocost...\n",
       "801905    #poblenou #barcelona #gentrification #gentrifi...\n",
       "801906    Saturday back to bocacocktaillounge from 23:00...\n",
       "801907    Just posted a photo  Barri de Gracia Barcelona...\n",
       "801908    Brew Pub to try a few of the 30 beers on offer...\n",
       "801909    I love my school is on fire! Fuego! Mantenlo p...\n",
       "801910    LAST DAY IN BARCELONA | Getting some sun some ...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check documents\n",
    "district_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second objective is to look at the dynamic topic development over time. For this purpose, the dataset is divided according to time and documents are created on this basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataframe according to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort index\n",
    "sorted_tweets = english_tweets.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-11 13:46:35\n",
      "2017-12-04 21:20:25\n"
     ]
    }
   ],
   "source": [
    "# check first and last date\n",
    "print(sorted_tweets.index[0]) # June 2017\n",
    "print(sorted_tweets.index[-1]) # December 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column that contains the month of the tweets\n",
    "sorted_tweets['month'] = sorted_tweets.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # split dataframe according to month\n",
    "# june = sorted_tweets.loc['2017-06-01':'2017-06-30']\n",
    "# july = sorted_tweets.loc['2017-07-01':'2017-07-31']\n",
    "# august = sorted_tweets.loc['2017-08-01':'2017-08-31']\n",
    "# september = sorted_tweets.loc['2017-09-01':'2017-09-30']\n",
    "# october = sorted_tweets.loc['2017-10-01':'2017-10-31']\n",
    "# november = sorted_tweets.loc['2017-11-01':'2017-11-30']\n",
    "# december = sorted_tweets.loc['2017-12-01':'2017-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from each month and treat them as one document respectively\n",
    "months_combined = english_tweets[[\"text_clean\",\"month\"]].groupby(\"month\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_month_pooling = months_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many documents\n",
    "len(documents_month_pooling) # should be 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge all tweets from one district and one month and treat them as one document respectively\n",
    "districts_per_month_combined = sorted_tweets[[\"text_clean\",\"month\",\"id_distrito\"]].groupby([\"month\",\"id_distrito\"])[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_per_month_pooling = districts_per_month_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many documents\n",
    "len(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month  id_distrito\n",
       "6      801901         Boy don't hurt your brain  Gothic Quarter Barc...\n",
       "       801902         I was moved  inspired  My life will never be t...\n",
       "       801903         CARAVANA #casualcomposition #waytosee #francba...\n",
       "       801904         JoelJoanJuveMorning concepts #everisdesign #de...\n",
       "       801905         Lots of walks some hikes sightseeing Sangria f...\n",
       "       801906         I like the shadow and light in this #iphone pi...\n",
       "       801907         Just posted a photo  Parc del Laberint d'Horta...\n",
       "       801908         Ella y el Agua! #karla #bcn #AJ #love #rocknro...\n",
       "       801909         Conmuting to work ( METRO Navas  tmb_barcelona...\n",
       "       801910          seeing him live was a religious experience #k...\n",
       "7      801901         More #beautifulmemories from #barcelona #artdi...\n",
       "       801902         Frederic Amat La instalación Zoótropo #welcome...\n",
       "       801903          Look for the noise hidden in silence the move...\n",
       "       801904         Just posted a photo  Palau De Congressos De Ba...\n",
       "       801905         Haciendo #retrospectiva y #feedback en el #mma...\n",
       "       801906         #travel #traveling #travelporn #travelmore #tr...\n",
       "       801907         ELENA ESTAUN Super Team \\nBarcelona Fashion We...\n",
       "       801908         And I will always love you\\nI will always love...\n",
       "       801909         I just finished running 820 km in 49m:23s with...\n",
       "       801910         Today we went to #DavidBowieIsBCN exhibit at t...\n",
       "8      801901         Out clubbing tonight #Dance #HipHop #Trap #Reg...\n",
       "       801902         Cuteness overload Me la como a besos  #martina...\n",
       "       801903         Last day in Barcelona Learning photography #ju...\n",
       "       801904         I just finished running 780 km in 57m:06s with...\n",
       "       801905         Summer mood the art of making a good gaspacho ...\n",
       "       801906         I was at Park Güell in Barcelona Excited about...\n",
       "       801907         Calor en rosa #pinkpower  Barcelona Horta Guin...\n",
       "       801908         How can you not want to jump for joy in such a...\n",
       "       801909         Vuela hasta romper la jaula o no vueles\\n\\n\\n\\...\n",
       "       801910         Do more of what makes you happy\\n\\n#couple #lo...\n",
       "                                            ...                        \n",
       "9      801903         Hospital de Sant Pau\\n#worldheritage \\n#spain ...\n",
       "       801904         prinxeMu Slybovyu That's my good robot sonBon ...\n",
       "       801905         Good Night \\n\\n #goodnight  #night #soñarbonit...\n",
       "       801906         Discovering new spots in Barcelona is always e...\n",
       "       801907         Love it \n",
       " Saturday \n",
       " mensfashionreview \n",
       " Bless...\n",
       "       801908         New tattoo!  Another big tattoo from risetatto...\n",
       "       801909         \\n\\n\\n\\n\\n #sneakerhead #todayskicks #kotd #sn...\n",
       "       801910         Reflections\\n#reflection #blue #barcelonaforum...\n",
       "10     801901         Getting lost with m31rk in Gothic Quarters \\n#...\n",
       "       801902         La Pedrera the stone quarry by Antoni Gaudí\\...\n",
       "       801903         #referendumRAC1 #CatalanReferendum2017 l'estac...\n",
       "       801904         This man is 84 years old He lived through the ...\n",
       "       801905         If you want to keep up to date with what is ha...\n",
       "       801906         Sometimes I think that people (especially thos...\n",
       "       801907         Nearly hometime feels #catwalk #rosebush #catu...\n",
       "       801908         Y después de votar en Meson can meli Just post...\n",
       "       801909         And I was like fuck that these are my colors...\n",
       "       801910         Ready tonight will be rocks!\\n#iloverockandrol...\n",
       "11     801901         Croissant crisis in France #watercolor #winsor...\n",
       "       801902         An unforgettable night  dondiablo Still one of...\n",
       "       801903         #celebrating with just a tiny bit of #glitter ...\n",
       "       801904         Date with Maja  Ennio #whitebitchez ( Ikibana ...\n",
       "       801905         Carnaroli rice with #foiegras and #seasinal mu...\n",
       "       801906         Vegan pizza!  ( Dolce Pizza  Los Veganos in Ba...\n",
       "       801907          #happyhalloween from #barcelona #spain #great...\n",
       "       801908         Nachetes ( Avocado in Barcelona Catalonia) Pla...\n",
       "       801909         PS I did have red hair and red lipstick but I'...\n",
       "       801910         What a luxurious feeling to be on the broadest...\n",
       "12     801907         Gaudi awesomeness  Basílica de la Sagrada Famí...\n",
       "       801908         Brama (at CrosmasLounge in Barcelona w/ terraf...\n",
       "Name: text_clean, Length: 62, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataframe\n",
    "districts_per_month_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training and Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('documents.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents:\n",
    "        f.write(item)\n",
    "with io.open('documents_user_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_user_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_hashtag_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_hashtag_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_district_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_month_pooling:\n",
    "        f.write(item)\n",
    "with io.open('documents_district_per_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_per_month_pooling:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_no_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents]\n",
    "\n",
    "texts_user_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_user_pooling]\n",
    "\n",
    "texts_hashtag_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing of Training Documents after Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove numbers, but not words that contain numbers.\n",
    "texts_no_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that are only one character.\n",
    "texts_no_pooling = [[token for token in doc if len(token) > 1] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if len(token) > 1] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if len(token) > 1] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize all words in all documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts_no_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part! computing bigrams did not improve models but made them worse!\n",
    "\n",
    "# # compute bigrams\n",
    "# # add bigrams and trigrams to docs (only ones that appear 5 times or more)\n",
    "# bigram = Phrases(texts_no_pooling, min_count=10)\n",
    "# for idx in range(len(texts_no_pooling)):\n",
    "#     for token in bigram[texts_no_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_no_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_user_pooling, min_count=10)\n",
    "# for idx in range(len(texts_user_pooling)):\n",
    "#     for token in bigram[texts_user_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_user_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_hashtag_pooling, min_count=10)\n",
    "# for idx in range(len(texts_hashtag_pooling)):\n",
    "#     for token in bigram[texts_hashtag_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_hashtag_pooling[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Tokenized Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenized_documents_no_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_no_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_user_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_user_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_hashtag_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_hashtag_pooling, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine and Vectorize Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to refine and vectorize corpus \n",
    "# (remove stopwords, very frequent and very infrequent words etc.)\n",
    "\n",
    "# define stopwords\n",
    "stpwords = 'for a of the and to in at by spain barcelona #barcelona #spain de la del en las \"barcelona #bcn'.split()\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, \n",
    "                additional_stopwords=set(stpwords),\n",
    "                no_below=2, no_above=0.5,\n",
    "                dictionary_name='tourism.dict', corpus_name='tourism.mm'):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    dictionary.compactify()\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    dictionary.save(dictionary_name)  # store the dictionary, for future reference\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize(corpus_name, corpus)  # store to disk, for later use\n",
    "    \n",
    "    return (corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:35,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:35,854 : INFO : built Dictionary(16741 unique tokens: ['#olgodbarcelona', '#triathlontraining', '#ølgod', 'beer', 'brew']...) from 7633 documents (total 79537 corpus positions)\n",
      "2018-09-26 16:37:36,032 : INFO : discarding 11562 tokens: [('#olgodbarcelona', 1), ('#triathlontraining', 1), ('quickie', 1), ('#hadthemostmusselsinmylife', 1), ('#tripoftheyear', 1), ('feast!', 1), ('freshest', 1), ('hail', 1), ('2h:27m:58s', 1), ('mallan', 1)]...\n",
      "2018-09-26 16:37:36,033 : INFO : keeping 5035 tokens which were in no less than 2 and no more than 3816 (=50.0%) documents\n",
      "2018-09-26 16:37:36,046 : INFO : resulting dictionary: Dictionary(5035 unique tokens: ['#ølgod', 'beer', 'brew', 'offer', 'pub']...)\n",
      "2018-09-26 16:37:36,053 : INFO : saving Dictionary object under tourism_no_pooling.dict, separately None\n",
      "2018-09-26 16:37:36,061 : INFO : saved tourism_no_pooling.dict\n",
      "2018-09-26 16:37:36,232 : INFO : storing corpus in Matrix Market format to tourism_no_pooling.mm\n",
      "2018-09-26 16:37:36,238 : INFO : saving sparse matrix to tourism_no_pooling.mm\n",
      "2018-09-26 16:37:36,240 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:36,285 : INFO : PROGRESS: saving document #1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:36,318 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:36,367 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:36,399 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-26 16:37:36,425 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-26 16:37:36,454 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-26 16:37:36,481 : INFO : PROGRESS: saving document #7000\n",
      "2018-09-26 16:37:36,499 : INFO : saved 7633x5035 matrix, density=0.108% (41550/38432155)\n",
      "2018-09-26 16:37:36,502 : INFO : saving MmCorpus index to tourism_no_pooling.mm.index\n",
      "2018-09-26 16:37:36,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-09-26 16:37:36,693 : INFO : built Dictionary(16741 unique tokens: ['#olgodbarcelona', '#triathlontraining', '#ølgod', 'beer', 'brew']...) from 7633 documents (total 79537 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:36,746 : INFO : discarding 11562 tokens: [('#olgodbarcelona', 1), ('#triathlontraining', 1), ('quickie', 1), ('#hadthemostmusselsinmylife', 1), ('#tripoftheyear', 1), ('feast!', 1), ('freshest', 1), ('hail', 1), ('2h:27m:58s', 1), ('mallan', 1)]...\n",
      "2018-09-26 16:37:36,749 : INFO : keeping 5035 tokens which were in no less than 2 and no more than 3816 (=50.0%) documents\n",
      "2018-09-26 16:37:36,759 : INFO : resulting dictionary: Dictionary(5035 unique tokens: ['#ølgod', 'beer', 'brew', 'offer', 'pub']...)\n",
      "2018-09-26 16:37:36,766 : INFO : saving Dictionary object under tourism_no_pooling.dict, separately None\n",
      "2018-09-26 16:37:36,772 : INFO : saved tourism_no_pooling.dict\n",
      "2018-09-26 16:37:36,887 : INFO : storing corpus in Matrix Market format to tourism_no_pooling.mm\n",
      "2018-09-26 16:37:36,890 : INFO : saving sparse matrix to tourism_no_pooling.mm\n",
      "2018-09-26 16:37:36,891 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:36,910 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-26 16:37:36,934 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:36,955 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:36,980 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-26 16:37:36,999 : INFO : PROGRESS: saving document #5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:37,027 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-26 16:37:37,047 : INFO : PROGRESS: saving document #7000\n",
      "2018-09-26 16:37:37,067 : INFO : saved 7633x5035 matrix, density=0.108% (41550/38432155)\n",
      "2018-09-26 16:37:37,071 : INFO : saving MmCorpus index to tourism_no_pooling.mm.index\n",
      "2018-09-26 16:37:37,081 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-09-26 16:37:37,249 : INFO : built Dictionary(16761 unique tokens: ['castellano', 'catalangov:', 'cataluña', 'consecuencia', 'country']...) from 4424 documents (total 79383 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:37,308 : INFO : discarding 12432 tokens: [('castellano', 1), ('catalangov:', 1), ('consecuencia', 1), ('exclusion', 1), ('instil', 1), ('linguistica', 1), ('mala', 1), ('politica', 1), ('practica', 1), ('shameful', 1)]...\n",
      "2018-09-26 16:37:37,310 : INFO : keeping 4185 tokens which were in no less than 2 and no more than 2212 (=50.0%) documents\n",
      "2018-09-26 16:37:37,324 : INFO : resulting dictionary: Dictionary(4185 unique tokens: ['cataluña', 'country', 'e', 'fear', 'police']...)\n",
      "2018-09-26 16:37:37,328 : INFO : saving Dictionary object under tourism_user_pooling.dict, separately None\n",
      "2018-09-26 16:37:37,336 : INFO : saved tourism_user_pooling.dict\n",
      "2018-09-26 16:37:37,428 : INFO : storing corpus in Matrix Market format to tourism_user_pooling.mm\n",
      "2018-09-26 16:37:37,431 : INFO : saving sparse matrix to tourism_user_pooling.mm\n",
      "2018-09-26 16:37:37,433 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:37,476 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-26 16:37:37,503 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:37,527 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:37,549 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-26 16:37:37,562 : INFO : saved 4424x4185 matrix, density=0.182% (33668/18514440)\n",
      "2018-09-26 16:37:37,565 : INFO : saving MmCorpus index to tourism_user_pooling.mm.index\n",
      "2018-09-26 16:37:37,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n",
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:37,739 : INFO : built Dictionary(16761 unique tokens: ['castellano', 'catalangov:', 'cataluña', 'consecuencia', 'country']...) from 4424 documents (total 79383 corpus positions)\n",
      "2018-09-26 16:37:37,801 : INFO : discarding 12432 tokens: [('castellano', 1), ('catalangov:', 1), ('consecuencia', 1), ('exclusion', 1), ('instil', 1), ('linguistica', 1), ('mala', 1), ('politica', 1), ('practica', 1), ('shameful', 1)]...\n",
      "2018-09-26 16:37:37,803 : INFO : keeping 4185 tokens which were in no less than 2 and no more than 2212 (=50.0%) documents\n",
      "2018-09-26 16:37:37,812 : INFO : resulting dictionary: Dictionary(4185 unique tokens: ['cataluña', 'country', 'e', 'fear', 'police']...)\n",
      "2018-09-26 16:37:37,817 : INFO : saving Dictionary object under tourism_user_pooling.dict, separately None\n",
      "2018-09-26 16:37:37,822 : INFO : saved tourism_user_pooling.dict\n",
      "2018-09-26 16:37:37,931 : INFO : storing corpus in Matrix Market format to tourism_user_pooling.mm\n",
      "2018-09-26 16:37:37,936 : INFO : saving sparse matrix to tourism_user_pooling.mm\n",
      "2018-09-26 16:37:37,943 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:37,985 : INFO : PROGRESS: saving document #1000\n",
      "2018-09-26 16:37:38,020 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:38,050 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:38,068 : INFO : PROGRESS: saving document #4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:38,089 : INFO : saved 4424x4185 matrix, density=0.182% (33668/18514440)\n",
      "2018-09-26 16:37:38,092 : INFO : saving MmCorpus index to tourism_user_pooling.mm.index\n",
      "2018-09-26 16:37:38,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:38,506 : INFO : built Dictionary(11970 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...) from 6198 documents (total 148488 corpus positions)\n",
      "2018-09-26 16:37:38,545 : INFO : discarding 1931 tokens: [('(out', 1), ('10k++)', 1), (\"advisor's\", 1), ('carot', 1), ('celebration', 1), ('forgotten', 1), ('garden!', 1), ('napa', 1), ('oriental', 1), ('skewer', 1)]...\n",
      "2018-09-26 16:37:38,547 : INFO : keeping 9904 tokens which were in no less than 2 and no more than 3099 (=50.0%) documents\n",
      "2018-09-26 16:37:38,563 : INFO : resulting dictionary: Dictionary(9904 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...)\n",
      "2018-09-26 16:37:38,576 : INFO : saving Dictionary object under tourism_hashtag_pooling.dict, separately None\n",
      "2018-09-26 16:37:38,585 : INFO : saved tourism_hashtag_pooling.dict\n",
      "2018-09-26 16:37:38,757 : INFO : storing corpus in Matrix Market format to tourism_hashtag_pooling.mm\n",
      "2018-09-26 16:37:38,759 : INFO : saving sparse matrix to tourism_hashtag_pooling.mm\n",
      "2018-09-26 16:37:38,762 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:38,796 : INFO : PROGRESS: saving document #1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:38,851 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:38,900 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:38,941 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-26 16:37:38,980 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-26 16:37:39,019 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-26 16:37:39,025 : INFO : saved 6198x9904 matrix, density=0.138% (84677/61384992)\n",
      "2018-09-26 16:37:39,027 : INFO : saving MmCorpus index to tourism_hashtag_pooling.mm.index\n",
      "2018-09-26 16:37:39,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:39,272 : INFO : built Dictionary(11970 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...) from 6198 documents (total 148488 corpus positions)\n",
      "2018-09-26 16:37:39,317 : INFO : discarding 1931 tokens: [('(out', 1), ('10k++)', 1), (\"advisor's\", 1), ('carot', 1), ('celebration', 1), ('forgotten', 1), ('garden!', 1), ('napa', 1), ('oriental', 1), ('skewer', 1)]...\n",
      "2018-09-26 16:37:39,319 : INFO : keeping 9904 tokens which were in no less than 2 and no more than 3099 (=50.0%) documents\n",
      "2018-09-26 16:37:39,335 : INFO : resulting dictionary: Dictionary(9904 unique tokens: ['#11setembre', '#2017', '#aj', '#aperitivo', '#architecture']...)\n",
      "2018-09-26 16:37:39,345 : INFO : saving Dictionary object under tourism_hashtag_pooling.dict, separately None\n",
      "2018-09-26 16:37:39,355 : INFO : saved tourism_hashtag_pooling.dict\n",
      "2018-09-26 16:37:39,524 : INFO : storing corpus in Matrix Market format to tourism_hashtag_pooling.mm\n",
      "2018-09-26 16:37:39,527 : INFO : saving sparse matrix to tourism_hashtag_pooling.mm\n",
      "2018-09-26 16:37:39,529 : INFO : PROGRESS: saving document #0\n",
      "2018-09-26 16:37:39,563 : INFO : PROGRESS: saving document #1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-26 16:37:39,624 : INFO : PROGRESS: saving document #2000\n",
      "2018-09-26 16:37:39,670 : INFO : PROGRESS: saving document #3000\n",
      "2018-09-26 16:37:39,707 : INFO : PROGRESS: saving document #4000\n",
      "2018-09-26 16:37:39,750 : INFO : PROGRESS: saving document #5000\n",
      "2018-09-26 16:37:39,791 : INFO : PROGRESS: saving document #6000\n",
      "2018-09-26 16:37:39,800 : INFO : saved 6198x9904 matrix, density=0.138% (84677/61384992)\n",
      "2018-09-26 16:37:39,802 : INFO : saving MmCorpus index to tourism_hashtag_pooling.mm.index\n"
     ]
    }
   ],
   "source": [
    "# run function to vectorize corpora\n",
    "corpus_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                corpus_name=\"tourism_no_pooling.mm\")[0]\n",
    "dictionary_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                    dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_no_pooling.mm\")[1]\n",
    "\n",
    "corpus_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[0]\n",
    "dictionary_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[1]\n",
    "\n",
    "corpus_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[0]\n",
    "dictionary_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Function to Preprocess Test Documents (Before Testing Them with LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function has to include all the same steps that were applied to the training documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def preprocess(docs):\n",
    "    ''' Conduct all preprocessing steps that are conducted to train the LDA model'''\n",
    "    \n",
    "    # tokenize documents\n",
    "    tokenized = [[word for word in document.lower().split()]\n",
    "          for document in docs]\n",
    "    \n",
    "    # remove words that are only one character\n",
    "    tokenized = [[token for token in doc if len(token) > 1] for doc in tokenized]\n",
    "    \n",
    "    # lemmatize all words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in tokenized]\n",
    "    \n",
    "    # define stopwords\n",
    "    stpwords = 'for a of the and to in at by spain barcelona #barcelona #spain de la del en las \"barcelona #bcn'.split()\n",
    "    \n",
    "    # get stopwords from nltk\n",
    "    def nltk_stopwords():\n",
    "        return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # combine stopwords\n",
    "    stopwords = nltk_stopwords().union(stpwords)\n",
    "    \n",
    "    # remove stopwords\n",
    "    preprocessed = [[token for token in document if token not in stopwords] for document in lemmatized]\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply function to test documents\n",
    "texts_district_pooling = preprocess(documents_district_pooling)\n",
    "texts_month_pooling = preprocess(documents_month_pooling)\n",
    "texts_district_per_month_pooling = preprocess(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenized_documents_district_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_month_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_district_per_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_per_month_pooling, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # map tokens to ids\n",
    "# print(dictionary_no_pooling.token2id)\n",
    "# print(dictionary_user_pooling.token2id)\n",
    "# print(dictionary_hashtag_pooling.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part!\n",
    "\n",
    "# # convert new document to vector \n",
    "# new_doc = \"Sagrada Familia is amazing\"\n",
    "# new_vec_no_pooling = dictionary_no_pooling.doc2bow(new_doc.lower().split())\n",
    "# print(new_vec_no_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ignore this part! not needed for dataset!\n",
    "\n",
    "# # corpus streaming: one document at a time\n",
    "# class MyCorpus(object):\n",
    "#     def __iter__(self):\n",
    "#         for line in open(\"corpus_no_pooling.txt\"):\n",
    "#             # assume there's one document per line, tokens separated by whitespace\n",
    "#             yield dictionary.doc2bow(line.lower().split())\n",
    "#             \n",
    "# corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "# print(corpus_memory_friendly)\n",
    "# \n",
    "# for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "#     print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
