{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/tut2.html#Gensim    \n",
    "# https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    "# https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# https://pypi.org/project/pyLDAvis/1.0.0/\n",
    "# https://gist.github.com/tokestermw/3588e6fbbb2f03f89798\n",
    "# https://stackoverflow.com/questions/11162402/lda-topic-modeling-training-and-testing\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "import pickle\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log events\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectorized Corpora and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_no_pooling.dict\")):\n",
    "   dictionary_no_pooling = corpora.Dictionary.load(\"../outputs/tourism_no_pooling.dict\")\n",
    "   corpus_no_pooling = corpora.MmCorpus(\"../outputs/tourism_no_pooling.mm\")\n",
    "   print(\"Vectorized no pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")\n",
    "\n",
    "# load user pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_user_pooling.dict\")):\n",
    "   dictionary_user_pooling = corpora.Dictionary.load(\"../outputs/tourism_user_pooling.dict\")\n",
    "   corpus_user_pooling = corpora.MmCorpus(\"../outputs/tourism_user_pooling.mm\")\n",
    "   print(\"Vectorized user pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")\n",
    "\n",
    "# load hashtag pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_hashtag_pooling.dict\")):\n",
    "   dictionary_hashtag_pooling = corpora.Dictionary.load(\"../outputs/tourism_hashtag_pooling.dict\")\n",
    "   corpus_hashtag_pooling = corpora.MmCorpus(\"../outputs/tourism_hashtag_pooling.mm\")\n",
    "   print(\"Vectorized hashtag pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../outputs/tokenized_documents_no_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_no_pooling = pickle.load(fp)\n",
    "with open (\"../outputs/tokenized_documents_user_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_user_pooling = pickle.load(fp)\n",
    "with open (\"../outputs/tokenized_documents_hashtag_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_hashtag_pooling = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LDA Models with Different Pooling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two evaluation metrics for topic models come to mind: coherence values and perplexity. Coherence values will be used to evaluate different LDA models (varying the number of topics) as this metric tends to favor better human interpretable topics (which is the objective of this research). The number of topics will be limited to 8 to avoid too much granularity. However, sometimes the highest coherence values do not give the most human interpretable topics. Visualization of the topic models can additionally help to understand and interprete the topics. The c_v measure will be used as a coherence measure to evaluate the LDA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train various LDA models with different number of topics\n",
    "# and evaluate their coherence values (choose the number of topics with the highest coherence value)\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit=9, start=4, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    model_topics = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model= models.LdaModel(corpus=corpus, id2word=dictionary, alpha='auto', eta='auto',\n",
    "                       eval_every=1, iterations=400, passes=20, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        model_topics = model.show_topics(formatted=False)\n",
    "    \n",
    "        model_topics = [[word for word, prob in topic] for topicid, topic in model_topics]\n",
    "    \n",
    "        coherencemodel = CoherenceModel(topics=model_topics, texts=texts, dictionary=dictionary, window_size=10)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return (model_list, coherence_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different no pooling models by running the function\n",
    "no_pooling_models = compute_coherence_values(dictionary=dictionary_no_pooling,\n",
    "                         corpus=corpus_no_pooling, texts=tokenized_documents_no_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coherence score of the different models\n",
    "model_list_no_pooling = no_pooling_models[0]\n",
    "coherence_values_no_pooling = no_pooling_models[1]\n",
    "\n",
    "limit=9; start=4; step=1;\n",
    "x = range(start, limit, step)\n",
    "_ = plt.plot(x, coherence_values_no_pooling)\n",
    "_ = plt.xlabel(\"Num Topics\")\n",
    "_ = plt.ylabel(\"Coherence score\")\n",
    "_ = plt.legend((\"coherence_values\"), loc='best')\n",
    "_ = plt.savefig(\"no_pooling_coherence_scores\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Choose the model with the highest coherence score (7 topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics of model with highest coherence score\n",
    "lda_model_no_pooling = model_list_no_pooling[3] # 7 topics model\n",
    "_ = lda_model_no_pooling.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize No Pooling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_np = pyLDAvis.gensim.prepare(lda_model_no_pooling, corpus_no_pooling, dictionary_no_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a topic trend is already visible in the no pooling model, the topics are a little bit mixed up and could be more interpretable. This finding can be attributed to the shortness of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different user pooling models by running the function\n",
    "user_pooling_models = compute_coherence_values(dictionary=dictionary_user_pooling,\n",
    "                         corpus=corpus_user_pooling, texts=tokenized_documents_user_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coherence score of the different models\n",
    "model_list_user_pooling = user_pooling_models[0]\n",
    "coherence_values_user_pooling = user_pooling_models[1]\n",
    "\n",
    "limit=9; start=4; step=1;\n",
    "x = range(start, limit, step)\n",
    "_ = plt.plot(x, coherence_values_user_pooling)\n",
    "_ = plt.xlabel(\"Num Topics\")\n",
    "_ = plt.ylabel(\"Coherence score\")\n",
    "_ = plt.legend((\"coherence_values\"), loc='best')\n",
    "_ = plt.savefig(\"user_pooling_coherence_scores\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model with the highest coherence score (8 topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics of model with highest coherence score\n",
    "lda_model_user_pooling = model_list_user_pooling[4] # 8 topics model\n",
    "_ = lda_model_user_pooling.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize User Pooling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_up = pyLDAvis.gensim.prepare(lda_model_user_pooling, corpus_user_pooling, dictionary_user_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the user pooling model look similar to the no pooling model. However, topics are even more mixed up and less interpretable since users tend to tweet about different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different hashtag pooling models by running the function\n",
    "hashtag_pooling_models = compute_coherence_values(dictionary=dictionary_hashtag_pooling,\n",
    "                         corpus=corpus_hashtag_pooling, texts=tokenized_documents_hashtag_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coherence score of the different models\n",
    "model_list_hashtag_pooling = hashtag_pooling_models[0]\n",
    "coherence_values_hashtag_pooling = hashtag_pooling_models[1]\n",
    "\n",
    "limit=9; start=4; step=1;\n",
    "x = range(start, limit, step)\n",
    "_ = plt.plot(x, coherence_values_hashtag_pooling)\n",
    "_ = plt.xlabel(\"Num Topics\")\n",
    "_ = plt.ylabel(\"Coherence score\")\n",
    "_ = plt.legend((\"coherence_values\"), loc='best')\n",
    "_ = plt.savefig(\"hashtag_pooling_coherence_scores\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model with the highest coherence score (7 topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print topics of model with highest coherence score\n",
    "lda_model_hashtag_pooling = model_list_hashtag_pooling[3] # 7 topics\n",
    "_ = lda_model_hashtag_pooling.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Hashtag Pooling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_hp = pyLDAvis.gensim.prepare(lda_model_hashtag_pooling, corpus_hashtag_pooling, dictionary_hashtag_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediary Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After training ~50 models for each pooling method, the following conclusion was reached: An inspection of the topics of the no pooling and user pooling method shows that they are less human interpretable than hashtag pooling models and show some repetitions in words among topics. Moreover, the no pooling model and user pooling model are very unstable as tweets are very short. The best trained hashtag pooling model (meaning the one with the most human interpretable topics) will therefore be saved and used for further purposes. Hashtag pooling was also shown to give the best results in various research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_no_pooling.save(\"../outputs/lda_model_no_pooling.model\") \n",
    "lda_model_user_pooling.save(\"../outputs/lda_model_user_pooling.model\") \n",
    "lda_model_hashtag_pooling.save(\"../outputs/lda_model_hashtag_pooling.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Whether TFIDF Can Improve LDA (Instead of BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes TFIDF improves LDA performance although LDA is mathematically meant to process a BOW input. TFIDF is therefore used to transform the corpus of the chosen model (hashtag pooling model with 7 topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tfidf model\n",
    "tfidf_hashtag_pooling = models.TfidfModel(corpus_hashtag_pooling)\n",
    "   \n",
    "# Run term frequency inverse document frequency transformation\n",
    "# (transform bag-of-words integer counts corpus to tfidf real-valued weights\n",
    "# corpus)\n",
    "corpus_tfidf_hashtag_pooling = tfidf_hashtag_pooling[corpus_hashtag_pooling]\n",
    "for doc in corpus_tfidf_hashtag_pooling:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hashtag pooling model with tfidf corpus\n",
    "lda_model_hashtag_pooling_tfidf = models.LdaModel(corpus_tfidf_hashtag_pooling,\n",
    "                                                  id2word=dictionary_hashtag_pooling,\n",
    "                                                  alpha='auto', eta='auto',\n",
    "                                                  eval_every=1,\n",
    "                                                  iterations=400, passes=20, num_topics=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trint topics of model\n",
    "_ = lda_model_hashtag_pooling_tfidf.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of tweets, however, TFIDF does not improve the results but makes them worse and less interpretable. Very rare terms are weighted heavier but in the case of tweets these seldomly have an interpretable topic (e.g. \"#youcanseeourhousefromhere\"). The model that will be used as final LDA model is thus the 7 topics hashtag pooling model applied to a BOW corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_hp = pyLDAvis.gensim.prepare(lda_model_hashtag_pooling, corpus_hashtag_pooling, dictionary_hashtag_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual inspection of the topics leads to the following labels:\n",
    "#### Topic 0: Sightseeing (Sagrada Familia, gaudi, architecture, travel, church ...)\n",
    "#### Topic 1: Summer, Sun & Friends (beach, friends, summer. smile, sun...)\n",
    "#### Topic 2: Streetart (graffiti, streetart, arte urbano, massive, streetphotography...)\n",
    "#### Topic 3: Everyday Life (yum, home, place, call, tapas ...)\n",
    "#### Topic 4: Lifestyle & Culture (yoga, selfie, contemporaryart, yummy, brianeno ...)\n",
    "#### Topic 5: Nightlife (night, olgod beer bar, cocktail, beer, raval ...) \n",
    "#### Topic 6: Sports, Health & Image (workout, fit, meditation, healthy, video ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tisplay the 10 most important words for each topic\n",
    "n_topics = 7\n",
    "topic_terms = []\n",
    "\n",
    "for i in range(0, n_topics):\n",
    "    temp = lda_model_hashtag_pooling.show_topic(i, 10)\n",
    "    terms = []\n",
    "    for term in temp:\n",
    "        terms.append(term)\n",
    "    topic_terms.append(terms)\n",
    "    print(\"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join([str(i[0]) for i in terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wordclouds for the topics\n",
    "def terms_to_wordcounts(terms, multiplier=1000):\n",
    "    return  \" \".join([\" \".join(int(multiplier*i[1]) * [i[0]]) for i in terms])\n",
    "\n",
    "wordclouds = []\n",
    "i = 0\n",
    "\n",
    "for topic in topic_terms:\n",
    "    wordcloud = WordCloud(background_color=\"black\", collocations=False).generate(terms_to_wordcounts(topic))\n",
    "    \n",
    "    _ = plt.imshow(wordcloud)\n",
    "    _ = plt.axis(\"off\")\n",
    "    _ = plt.savefig(\"terms_wordcloud_topic\" + str(i))\n",
    "    _ = plt.show()\n",
    "    \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esade-topic-analysis-twitter",
   "language": "python",
   "name": "esade-topic-analysis-twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
