{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "# http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "# https://github.com/derekgreene/topic-model-tutorial/blob/master/3%20-%20Parameter%20Selection%20for%20NMF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gensim.models\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import combinations\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display graphs in jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectorized Corpora and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_no_pooling.dict\")):\n",
    "   dictionary_no_pooling = corpora.Dictionary.load(\"../outputs/tourism_no_pooling.dict\")\n",
    "   corpus_no_pooling = corpora.MmCorpus(\"../outputs/tourism_no_pooling.mm\")\n",
    "   print(\"Vectorized no pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents and Unpreprocessed Tokenized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no pooling documents\n",
    "with open (\"../outputs/nmf_documents_no_pooling.p\", \"rb\") as fp:\n",
    "    documents_no_pooling = pickle.load(fp)\n",
    "\n",
    "# load no pooling unpreprocessed tokenized documents\n",
    "with open (\"../outputs/tokenized_documents_no_pooling_unpp.p\", \"rb\") as fp:\n",
    "    tokenized_documents_no_pooling = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf is usually used before running NMF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "tfidf_no_pooling = tfidf_vectorizer.fit_transform(documents_no_pooling)\n",
    "tfidf_feature_names_no_pooling = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NMF Model and Determine Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of possible topic numbers\n",
    "kmin, kmax = 4, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF model for different number of topics\n",
    "topic_models = []\n",
    "# Try each value of k\n",
    "for k in range(kmin,kmax+1):\n",
    "    print(\"Applying NMF for k=%d ...\" % k)\n",
    "    # run NMF\n",
    "    model = decomposition.NMF( init=\"nndsvd\", n_components=k) \n",
    "    W = model.fit_transform(tfidf_no_pooling)\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append((k,W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized documents\n",
    "tokenized_documents_no_pooling = [[item.replace(\"#\",\"\") for item in document] for document in tokenized_documents_no_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokenized documents\n",
    "tokenized_documents_no_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_documents_no_pooling, min_count=2, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate coherence\n",
    "def calculate_coherence(w2v_model, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(w2v_model.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get top terms\n",
    "def get_descriptor(all_terms, H, topic_index, top):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort(H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(all_terms[term_index])\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run functions\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append(get_descriptor(tfidf_feature_names_no_pooling, H, topic_index, 10))\n",
    "    # now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append(calculate_coherence(w2v_model,term_rankings))\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph settings\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "fig = plt.figure(figsize=(13,7))\n",
    "\n",
    "# Create the line plot\n",
    "ax = plt.plot( k_values, coherences )\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "\n",
    "# Add the points\n",
    "plt.scatter( k_values, coherences, s=120)\n",
    "\n",
    "# Find and annotate the maximum point on the plot\n",
    "ymax = max(coherences)\n",
    "xpos = coherences.index(ymax)\n",
    "best_k = k_values[xpos]\n",
    "plt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
    "plt.savefig(\"coherence_scores_nmf\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best number of topics according to tc-w2v coherence measure\n",
    "k = best_k\n",
    "\n",
    "# Get the model that we generated earlier.\n",
    "W = topic_models[k-kmin][1]\n",
    "H = topic_models[k-kmin][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top words for each topic\n",
    "for topic_index in range(k):\n",
    "    descriptor = get_descriptor(tfidf_feature_names_no_pooling, H, topic_index, 20)\n",
    "    str_descriptor = \", \".join( descriptor )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Upon inspection, NMF works very well (seems to be more interpretable than the LDA no pooling model). However, the topics are very repetitive and not as clearly interpretable and differentiatable as the topcis detected by the LDA model with hashtag pooling. The problem of repetitions of keywords among different topics complicates the matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esade-topic-analysis-twitter",
   "language": "python",
   "name": "esade-topic-analysis-twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
