{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/tut1.html\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"C:\\Users\\s.birk\\Anaconda3\\envs\\esade-topic-analysis-twitter\\python.exe\"\n  * The NumPy version is: \"1.20.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\numpy\\core\\multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[0;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0aa0b027fcb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[0;32m     47\u001b[0m         __version__, exc)\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"C:\\Users\\s.birk\\Anaconda3\\envs\\esade-topic-analysis-twitter\\python.exe\"\n  * The NumPy version is: \"1.20.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"C:\\Users\\s.birk\\Anaconda3\\envs\\esade-topic-analysis-twitter\\python.exe\"\n  * The NumPy version is: \"1.20.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cdbd742ce335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\esade-topic-analysis-twitter\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     raise ImportError(\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;34m\"Unable to import required dependencies:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdependency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"C:\\Users\\s.birk\\Anaconda3\\envs\\esade-topic-analysis-twitter\\python.exe\"\n  * The NumPy version is: \"1.20.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import io\n",
    "import logging\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords and lemmatizer from nltk package\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log events\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data with timestamp as index\n",
    "tweets = pd.read_csv(\"tweets.csv\", encoding=\"latin1\", parse_dates=True, \n",
    "                     index_col=\"created\", usecols=range(1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe info\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe dataframe \n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset according to language: extract english language\n",
    "english_tweets = tweets[tweets[\"language3\"] == \"ENGLISH\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Twitter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display text\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "english_tweets[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text\"].str.replace(r\"http\\S+\", \"\")\n",
    "# remove emoticons\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"<.*>\", \"\")\n",
    "# remove punctuation, special characters etc.\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"&amp\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\.\", \" \")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\,\", \" \")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\;\", \" \")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\-\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\\"\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(\"\\\\\\\\\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(\"\\/\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(\"\\*\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"@\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\n\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\|\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"W//\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"!\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"~\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\")\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"(\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"?\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\":\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\{\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"\\}\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"_\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"h:[0-9]+m:[0-9]+s\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[0-9]+\", \" \")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"w/\", \" \")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x97]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x96]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x95]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x94]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x93]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x92]+\", \"'\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x91]+\", \"\")\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\"[\\x85]+\", \"\")\n",
    "\n",
    "# reduce white spaces to 1\n",
    "english_tweets[\"text_clean\"] = english_tweets[\"text_clean\"].str.replace(r\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display cleaned text\n",
    "english_tweets[\"text_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "english_tweets.columns\n",
    "cols = ['text', 'text_clean', 'favoriteCount', 'replyToSN', 'truncated', 'replyToSID',\n",
    "       'replyToUID', 'statusSource', 'retweetCount', 'longitude', 'latitude',\n",
    "       'id_seccion', 'horaPeticion', 'id_distrito', 'grupoHora',\n",
    "       'id_seccion_xy', 'favoriteCountOutlier', 'retweetCountOutlier',\n",
    "       'tweetcount', 'movement', 'language3', 'dayofweek', 'weeknumber',\n",
    "       'month', 'idBarrio_xy', 'idBarrio', 'user']\n",
    "\n",
    "english_tweets = english_tweets[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to train the topic models, 3 different pooling methods for the creation of documents are used: No Pooling (1), User Pooling (2) and Hashtag Pooling (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Documents Option 1 (No Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat every tweet as a different document (no pooling)\n",
    "documents = english_tweets[\"text_clean\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Documents Option 2 (User Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat all tweets by one user as one single document (user pooling)\n",
    "user_combined = english_tweets[[\"text_clean\",\"user\"]].groupby(\"user\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_user_pooling = user_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Documents Option 3 (Hashtag Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat all tweets with the same hashtag as one single document (hashtag pooling)\n",
    "\n",
    "# find all hashtags\n",
    "english_tweets[\"hashtags\"] = english_tweets[\"text_clean\"].str.findall(r'#.*?(?=\\s|$)')\n",
    "\n",
    "# separate hashtags in columns\n",
    "hashtags_tweets = pd.DataFrame(english_tweets[\"hashtags\"].tolist(),\n",
    "                               columns=[\"hashtag1\", \"hashtag2\", \"hashtag3\", \"hashtag4\",\n",
    "                                        \"hashtag5\", \"hashtag6\", \"hashtag7\", \"hashtag8\",\n",
    "                                        \"hashtag9\", \"hashtag10\", \"hashtag11\", \"hashtag12\",\n",
    "                                        \"hashtag13\"])\n",
    "\n",
    "# join hashtags with tweet text\n",
    "hashtags_tweets.index = english_tweets.index\n",
    "hashtags_tweets = english_tweets.join(hashtags_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one dataframe with text for each hashtag column and save them in a dictionary\n",
    "dict = {}\n",
    "for index, item in enumerate([\"hash1\", \"hash2\", \"hash3\", \"hash4\", \"hash5\",\n",
    "                              \"hash6\", \"hash7\", \"hash8\", \"hash9\", \"hash10\",\n",
    "                              \"hash11\", \"hash12\", \"hash13\"]):\n",
    "    dict[item] = hashtags_tweets[[\"hashtag\" + str(index + 1), \"text_clean\"]].copy()\n",
    "    dict[item].columns = [\"hashtag\", \"text\"]\n",
    "    dict[item].dropna(inplace=True)\n",
    "\n",
    "# concatenate all dataframes to one dataframe (the result is a dataframe\n",
    "# where there is text for each hashtag found)\n",
    "hashtags = pd.DataFrame()\n",
    "for item in dict:\n",
    "    hashtags = pd.concat([hashtags, dict[item]])\n",
    "\n",
    "# combine text for each hashtag\n",
    "hashtags_combined = hashtags.groupby(\"hashtag\")[\"text\"].apply(lambda x: \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some generic hashtags that cover a lot of different topics\n",
    "hashtags_combined.drop([\"#Barcelona\", \"#Catalunya\", \"#Spain\", \"#BCN\", \"#BARCELONA\",\n",
    "                        \"#Espana\", \"#BarcelonaSpain\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create documents\n",
    "documents_hashtag_pooling = hashtags_combined.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of terms per document\n",
    "total = []\n",
    "for item in documents:\n",
    "    total.append(len(item.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of terms\n",
    "sum = 0\n",
    "for number in total:\n",
    "    sum += number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of terms per document\n",
    "sum/len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The trained topic models will then be used to determine the topics of test documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first objective of the research is to analyze the distribution of topics over the districts. For this purpose, district pooling is used to create the documents that will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from each district (district pooling) and treat them as one single document respectively\n",
    "district_combined = english_tweets[[\"text_clean\",\"id_distrito\"]].groupby(\"id_distrito\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_pooling = district_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check documents\n",
    "district_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if order is right\n",
    "district_combined.index # correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second objective is to look at the dynamic topic development over time. For this purpose, the dataset is divided according to time and documents are created on this basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataframe according to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort index\n",
    "sorted_tweets = english_tweets.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first and last date\n",
    "print(sorted_tweets.index[0]) # June 2017\n",
    "print(sorted_tweets.index[-1]) # December 2017 (very incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column that contains the month of the tweets\n",
    "sorted_tweets['month'] = sorted_tweets.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe according to month\n",
    "june = sorted_tweets.loc['2017-06-01':'2017-06-30']\n",
    "july = sorted_tweets.loc['2017-07-01':'2017-07-31']\n",
    "august = sorted_tweets.loc['2017-08-01':'2017-08-31']\n",
    "september = sorted_tweets.loc['2017-09-01':'2017-09-30']\n",
    "october = sorted_tweets.loc['2017-10-01':'2017-10-31']\n",
    "november = sorted_tweets.loc['2017-11-01':'2017-11-30']\n",
    "december = sorted_tweets.loc['2017-12-01':'2017-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of tweets per month\n",
    "len(june)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from each month and treat them as one document respectively\n",
    "months_combined = english_tweets[[\"text_clean\",\"month\"]].groupby(\"month\")[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_month_pooling = months_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many documents\n",
    "len(documents_month_pooling) # should be 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check order of documents\n",
    "months_combined.index # Aug, Dec, Jul, Jun, Nov, Oct, Sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters\n",
    "len(documents_month_pooling[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to wordlist\n",
    "aug_words = documents_month_pooling[0].split()\n",
    "dec_words = documents_month_pooling[1].split()\n",
    "jul_words = documents_month_pooling[2].split()\n",
    "jun_words = documents_month_pooling[3].split()\n",
    "nov_words = documents_month_pooling[4].split()\n",
    "oct_words = documents_month_pooling[5].split()\n",
    "sept_words = documents_month_pooling[6].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words per month\n",
    "print(len(jun_words))\n",
    "print(len(jul_words))\n",
    "print(len(aug_words))\n",
    "print(len(sept_words))\n",
    "print(len(oct_words))\n",
    "print(len(nov_words))\n",
    "print(len(dec_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique words\n",
    "print(len(set(jun_words)))\n",
    "print(len(set(jul_words)))\n",
    "print(len(set(aug_words)))\n",
    "print(len(set(sept_words)))\n",
    "print(len(set(oct_words)))\n",
    "print(len(set(nov_words)))\n",
    "print(len(set(dec_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from one district and one month and treat them as one document respectively\n",
    "districts_per_month_combined = sorted_tweets[[\"text_clean\",\"month\",\"id_distrito\"]].groupby([\"month\",\"id_distrito\"])[\"text_clean\"].apply(lambda x: \"\".join(x))\n",
    "documents_district_per_month_pooling = districts_per_month_combined.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check order of documents\n",
    "districts_per_month_combined.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many documents\n",
    "len(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "districts_per_month_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Documents for NMF Topic Modeling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of no pooling documents\n",
    "nmf_documents = list(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to lower case\n",
    "for doc_idx, doc in enumerate(nmf_documents):\n",
    "    nmf_documents[doc_idx] = nmf_documents[doc_idx].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords\n",
    "for doc_idx, doc in enumerate(nmf_documents):\n",
    "    nmf_documents[doc_idx] = doc.replace(\" year \", \" \").replace(\" to \", \" \").replace(\" on \", \" \").replace(\" wa \", \" \").replace(\" #yourup \", \" \").replace(\" de \", \" \").replace(\" just \", \" \").replace(\" posted \", \" \").replace(\" photo \", \" \").replace(\" la \", \" \").replace(\" del \", \" \").replace(\" en \", \" \").replace(\" los \", \" \").replace(\" el \", \" \").replace(\" las \", \" \").replace(\" barcelona \", \" \").replace(\" #bcn \", \" \").replace(\" just \", \" \").replace(\" cada \", \" \").replace(\" nuestra \", \" \").replace(\" around \", \" \").replace(\" spanish \", \" \").replace(\" día \", \" \").replace(\" dia \", \" \").replace(\" #photo \", \" \").replace(\" first \", \" \").replace(\" thing \", \" \").replace(\" last \", \" \").replace(\" #spain \", \" \").replace(\" carrer \", \" \").replace(\" make \", \" \").replace(\" &lt \", \" \").replace(\" &gt \", \" \").replace(\" de \", \" \").replace(\" for \", \" \").replace(\" a \", \" \").replace(\" of \", \" \").replace(\" the \", \" \").replace(\" and \", \" \").replace(\" to \", \" \").replace(\" in \", \" \").replace(\" at \", \" \").replace(\" by \", \" \").replace(\" one \", \" \").replace(\" day \", \" \").replace(\" get \", \" \").replace(\" españa \", \" \").replace(\" #españa \", \" \").replace(\" #repost \", \" \").replace(\" since \", \" \").replace(\" still \", \" \").replace(\" never \", \" \").replace(\" thank \", \" \").replace(\" two \", \" \").replace(\" think \", \" \").replace(\" could \", \" \").replace(\" many \", \" \").replace(\" even \", \" \").replace(\" the \", \" \").replace(\" igers \", \" \").replace(\" que \", \" \").replace(\" many \", \" \").replace(\" con \", \" \").replace(\" un \", \" \").replace(\" wa \", \" \").replace(\" bcn \", \" \").replace(\" d'horta \", \" \").replace(\" ever \", \" \").replace(\" come \", \" \").replace(\" #ig \", \" \").replace(\" el \", \" \").replace(\" i'm \", \" \").replace(\" i've \", \" \").replace(\" always \", \" \").replace(\" le \", \" \").replace(\" what's \", \" \").replace(\" #barcelone \", \" \").replace(\" like \", \" \").replace(\" last \", \" \").replace(\" back \", \" \").replace(\" thanks \", \" \").replace(\" #barna \", \" \").replace(\" spain \", \" \").replace(\" yo \", \" \").replace(\" #yo \", \" \").replace(\" el \", \" \").replace(\" #el \", \" \").replace(\" barcelona \", \" \").replace(\" #barcelona \", \" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display preprocessed documents\n",
    "nmf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training and Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('documents.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents:\n",
    "        f.write(item + \"\\n\")\n",
    "with io.open('documents_user_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_user_pooling:\n",
    "        f.write(item + \"\\n\")\n",
    "with io.open('documents_hashtag_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_hashtag_pooling:\n",
    "        f.write(item + \"\\n\")\n",
    "with io.open('documents_district_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_pooling:\n",
    "        f.write(item + \"\\n\")\n",
    "with io.open('documents_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_month_pooling:\n",
    "        f.write(item + \"\\n\")\n",
    "with io.open('documents_district_per_month_pooling.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in documents_district_per_month_pooling:\n",
    "        f.write(item + \"\\n\")\n",
    "        \n",
    "with open('documents_no_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(documents, fp)\n",
    "    \n",
    "with open('documents_user_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(documents_user_pooling, fp)\n",
    "    \n",
    "with open('documents_hashtag_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(documents_hashtag_pooling, fp)\n",
    "    \n",
    "with open('nmf_documents_no_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(nmf_documents, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can simply tokenize by space thanks to the previous preprocessing\n",
    "texts_no_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents]\n",
    "\n",
    "texts_user_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_user_pooling]\n",
    "\n",
    "texts_hashtag_pooling = [[word for word in document.lower().split()]\n",
    "          for document in documents_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Unpreprocessed Tokenized Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_documents_no_pooling_unpp.p', 'wb') as fp:\n",
    "    pickle.dump(texts_no_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_user_pooling_unpp.p', 'wb') as fp:\n",
    "    pickle.dump(texts_user_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_hashtag_pooling_unpp.p', 'wb') as fp:\n",
    "    pickle.dump(texts_hashtag_pooling, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing of Training Documents after Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers, but not words that contain numbers.\n",
    "texts_no_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if not token.isnumeric()] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that are only one character.\n",
    "texts_no_pooling = [[token for token in doc if len(token) > 1] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[token for token in doc if len(token) > 1] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[token for token in doc if len(token) > 1] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize all words in all documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts_no_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_no_pooling]\n",
    "texts_user_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_user_pooling]\n",
    "texts_hashtag_pooling = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ignore this part! computing bigrams did not improve models but made them worse!\n",
    "\n",
    "# # compute bigrams\n",
    "# # add bigrams and trigrams to docs (only ones that appear 5 times or more)\n",
    "# bigram = Phrases(texts_no_pooling, min_count=10)\n",
    "# for idx in range(len(texts_no_pooling)):\n",
    "#     for token in bigram[texts_no_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_no_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_user_pooling, min_count=10)\n",
    "# for idx in range(len(texts_user_pooling)):\n",
    "#     for token in bigram[texts_user_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_user_pooling[idx].append(token)\n",
    "#             \n",
    "# bigram = Phrases(texts_hashtag_pooling, min_count=10)\n",
    "# for idx in range(len(texts_hashtag_pooling)):\n",
    "#     for token in bigram[texts_hashtag_pooling[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_hashtag_pooling[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Tokenized Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_documents_no_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_no_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_user_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_user_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_hashtag_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_hashtag_pooling, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine and Vectorize Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to refine and vectorize corpus \n",
    "# (remove stopwords, very frequent and very infrequent words etc.)\n",
    "\n",
    "# define stopwords\n",
    "stpwords = \"for a of the and to in at by one #yo #el day get españa #yourup #españa #repost yo el since still never thank two think could many even ha igers th que con un wa bcn d'horta ever come #ig el i'm i've always le what's #barcelone like last back thanks #barna spain barcelona #barcelona cada nuestra around spanish día dia #photo first thing last #spain carrer make &lt &gt de la del en las barcelona #bcn just posted photo year wa\".split()\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, \n",
    "                additional_stopwords=set(stpwords),\n",
    "                no_below=2, no_above=0.5,\n",
    "                dictionary_name='tourism.dict', corpus_name='tourism.mm'):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    dictionary.compactify()\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    dictionary.save(dictionary_name)  # store the dictionary, for future reference\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize(corpus_name, corpus)  # store to disk, for later use\n",
    "    \n",
    "    return (corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function to vectorize corpora\n",
    "corpus_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                corpus_name=\"tourism_no_pooling.mm\")[0]\n",
    "dictionary_no_pooling = prep_corpus(texts_no_pooling,\n",
    "                                    dictionary_name=\"tourism_no_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_no_pooling.mm\")[1]\n",
    "\n",
    "corpus_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[0]\n",
    "dictionary_user_pooling = prep_corpus(texts_user_pooling,\n",
    "                                    dictionary_name=\"tourism_user_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_user_pooling.mm\")[1]\n",
    "\n",
    "corpus_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[0]\n",
    "dictionary_hashtag_pooling = prep_corpus(texts_hashtag_pooling,\n",
    "                                    dictionary_name=\"tourism_hashtag_pooling.dict\",\n",
    "                                    corpus_name=\"tourism_hashtag_pooling.mm\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Function to Preprocess Test Documents (Before Testing Them with Topic Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function has to include all the same steps that were applied to the training documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def preprocess(docs):\n",
    "    ''' Conduct all preprocessing steps that are conducted to train the LDA model'''\n",
    "    \n",
    "    # tokenize documents\n",
    "    tokenized = [[word for word in document.lower().split()]\n",
    "          for document in docs]\n",
    "    \n",
    "    # remove words that are only one character\n",
    "    tokenized = [[token for token in doc if len(token) > 1] for doc in tokenized]\n",
    "    \n",
    "    # lemmatize all words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in tokenized]\n",
    "    \n",
    "    # define stopwords\n",
    "    stpwords = \"for a of the and to in at by one #yo #el day get #yourup españa #españa #repost yo el since still never thank two think could many even ha igers th que con un wa bcn d'horta ever come #ig el i'm i've always le what's #barcelone like last back thanks #barna spain barcelona #barcelona cada nuestra around spanish día dia #photo first thing last #spain carrer make &lt &gt de la del en las barcelona #bcn just posted photo year wa\".split()\n",
    "    \n",
    "    # get stopwords from nltk\n",
    "    def nltk_stopwords():\n",
    "        return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # combine stopwords\n",
    "    stopwords = nltk_stopwords().union(stpwords)\n",
    "    \n",
    "    # remove stopwords\n",
    "    preprocessed = [[token for token in document if token not in stopwords] for document in lemmatized]\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to test documents\n",
    "texts_district_pooling = preprocess(documents_district_pooling)\n",
    "texts_month_pooling = preprocess(documents_month_pooling)\n",
    "texts_district_per_month_pooling = preprocess(documents_district_per_month_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Test Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_documents_district_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_month_pooling, fp)\n",
    "    \n",
    "with open('tokenized_documents_district_per_month_pooling.p', 'wb') as fp:\n",
    "    pickle.dump(texts_district_per_month_pooling, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ignore this part! just example code!\n",
    "\n",
    "# # map tokens to ids\n",
    "# print(dictionary_no_pooling.token2id)\n",
    "# print(dictionary_user_pooling.token2id)\n",
    "# print(dictionary_hashtag_pooling.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ignore this part! just example code!\n",
    "\n",
    "# # convert new document to vector \n",
    "# new_doc = \"Sagrada Familia is amazing\"\n",
    "# new_vec_no_pooling = dictionary_no_pooling.doc2bow(new_doc.lower().split())\n",
    "# print(new_vec_no_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ignore this part! not needed for dataset!\n",
    "\n",
    "# # corpus streaming: one document at a time\n",
    "# class MyCorpus(object):\n",
    "#     def __iter__(self):\n",
    "#         for line in open(\"corpus_no_pooling.txt\"):\n",
    "#             # assume there's one document per line, tokens separated by whitespace\n",
    "#             yield dictionary.doc2bow(line.lower().split())\n",
    "#             \n",
    "# corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "# print(corpus_memory_friendly)\n",
    "# \n",
    "# for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "#     print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esade-topic-analysis-twitter",
   "language": "python",
   "name": "esade-topic-analysis-twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
