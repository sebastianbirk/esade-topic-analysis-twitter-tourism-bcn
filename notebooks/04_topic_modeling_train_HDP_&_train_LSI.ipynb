{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html\n",
    "# https://medium.com/square-corner-blog/topic-modeling-optimizing-for-human-interpretability-48a81f6ce0ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from gensim import corpora, models\n",
    "from gensim.models import HdpModel, LsiModel, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log events\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpora and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_no_pooling.dict\")):\n",
    "   dictionary_no_pooling = corpora.Dictionary.load(\"../outputs/tourism_no_pooling.dict\")\n",
    "   corpus_no_pooling = corpora.MmCorpus(\"../outputs/tourism_no_pooling.mm\")\n",
    "   print(\"Vectorized no pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")\n",
    "\n",
    "# Load user pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_user_pooling.dict\")):\n",
    "   dictionary_user_pooling = corpora.Dictionary.load(\"../outputs/tourism_user_pooling.dict\")\n",
    "   corpus_user_pooling = corpora.MmCorpus(\"../outputs/tourism_user_pooling.mm\")\n",
    "   print(\"Vectorized user pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")\n",
    "\n",
    "# Load hashtag pooling corpus\n",
    "if (os.path.exists(\"../outputs/tourism_hashtag_pooling.dict\")):\n",
    "   dictionary_hashtag_pooling = corpora.Dictionary.load(\"../outputs/tourism_hashtag_pooling.dict\")\n",
    "   corpus_hashtag_pooling = corpora.MmCorpus(\"../outputs/tourism_hashtag_pooling.mm\")\n",
    "   print(\"Vectorized hashtag pooling corpus loaded!\")\n",
    "else:\n",
    "   print(\"Please run preprocessing script first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no pooling documents\n",
    "with open (\"../outputs/tokenized_documents_no_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_no_pooling = pickle.load(fp)\n",
    "\n",
    "# Load user pooling documents\n",
    "with open(\"../outputs/tokenized_documents_user_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_user_pooling = pickle.load(fp)\n",
    "\n",
    "# Load hashtag pooling documents\n",
    "with open(\"../outputs/tokenized_documents_hashtag_pooling.p\", \"rb\") as fp:\n",
    "    tokenized_documents_hashtag_pooling = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDA Models (Trained in 02 Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "lda_model_no_pooling = models.LdaModel.load(\"../outputs/lda_model_no_pooling.model\") # 6 topics\n",
    "lda_model_user_pooling = models.LdaModel.load(\"../outputs/lda_model_user_pooling.model\") # 7 topics\n",
    "lda_model_hashtag_pooling = models.LdaModel.load(\"../outputs/lda_model_hashtag_pooling.model\") # 7 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics with word probabilities\n",
    "lda_topics_no_pooling = lda_model_no_pooling.show_topics(formatted=False)\n",
    "lda_topics_user_pooling = lda_model_user_pooling.show_topics(formatted=False)\n",
    "lda_topics_hashtag_pooling = lda_model_hashtag_pooling.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train HDP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HDP models with different pooling methods (similar to LDA in 02 notebook)\n",
    "hdp_model_no_pooling = HdpModel(corpus_no_pooling, dictionary_no_pooling)\n",
    "hdp_model_user_pooling = HdpModel(corpus_user_pooling, dictionary_user_pooling)\n",
    "hdp_model_hashtag_pooling = HdpModel(corpus_hashtag_pooling, dictionary_hashtag_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics with word probabilities\n",
    "hdp_topics_no_pooling = hdp_model_no_pooling.show_topics(formatted=False)\n",
    "hdp_topics_user_pooling = hdp_model_user_pooling.show_topics(formatted=False)\n",
    "hdp_topics_hashtag_pooling = hdp_model_hashtag_pooling.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSI models with similar configurations as LDA models\n",
    "lsi_model_no_pooling = LsiModel(corpus=corpus_no_pooling, num_topics=6, id2word=dictionary_no_pooling)\n",
    "lsi_model_user_pooling = LsiModel(corpus=corpus_user_pooling, num_topics=7, id2word=dictionary_user_pooling)\n",
    "lsi_model_hashtag_pooling = LsiModel(corpus=corpus_hashtag_pooling, num_topics=7, id2word=dictionary_hashtag_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics with word probabilities\n",
    "lsi_topics_no_pooling = lsi_model_no_pooling.show_topics(formatted=False)\n",
    "lsi_topics_user_pooling = lsi_model_user_pooling.show_topics(formatted=False)\n",
    "lsi_topics_hashtag_pooling = lsi_model_hashtag_pooling.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Coherence Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top topic words\n",
    "lsi_topics_no_pooling = [[word for word, prob in topic] for topicid, topic in lsi_topics_no_pooling]\n",
    "lsi_topics_user_pooling = [[word for word, prob in topic] for topicid, topic in lsi_topics_user_pooling]\n",
    "lsi_topics_hashtag_pooling = [[word for word, prob in topic] for topicid, topic in lsi_topics_hashtag_pooling]\n",
    "\n",
    "hdp_topics_no_pooling = [[word for word, prob in topic] for topicid, topic in hdp_topics_no_pooling]\n",
    "hdp_topics_user_pooling = [[word for word, prob in topic] for topicid, topic in hdp_topics_user_pooling]\n",
    "hdp_topics_hashtag_pooling = [[word for word, prob in topic] for topicid, topic in hdp_topics_hashtag_pooling]\n",
    "\n",
    "lda_topics_no_pooling = [[word for word, prob in topic] for topicid, topic in lda_topics_no_pooling]\n",
    "lda_topics_user_pooling = [[word for word, prob in topic] for topicid, topic in lda_topics_user_pooling]\n",
    "lda_topics_hashtag_pooling = [[word for word, prob in topic] for topicid, topic in lda_topics_hashtag_pooling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherences\n",
    "lsi_no_pooling_coherence = CoherenceModel(topics=lsi_topics_no_pooling, texts=tokenized_documents_no_pooling, dictionary=dictionary_no_pooling, window_size=10).get_coherence()\n",
    "lsi_user_pooling_coherence = CoherenceModel(topics=lsi_topics_user_pooling, texts=tokenized_documents_user_pooling, dictionary=dictionary_user_pooling, window_size=10).get_coherence()\n",
    "lsi_hashtag_pooling_coherence = CoherenceModel(topics=lsi_topics_hashtag_pooling, texts=tokenized_documents_hashtag_pooling, dictionary=dictionary_hashtag_pooling, window_size=10).get_coherence()\n",
    "\n",
    "hdp_no_pooling_coherence = CoherenceModel(topics=hdp_topics_no_pooling, texts=tokenized_documents_no_pooling, dictionary=dictionary_no_pooling, window_size=10).get_coherence()\n",
    "hdp_user_pooling_coherence = CoherenceModel(topics=hdp_topics_user_pooling, texts=tokenized_documents_user_pooling, dictionary=dictionary_user_pooling, window_size=10).get_coherence()\n",
    "hdp_hashtag_pooling_coherence = CoherenceModel(topics=hdp_topics_hashtag_pooling, texts=tokenized_documents_hashtag_pooling, dictionary=dictionary_hashtag_pooling, window_size=10).get_coherence()\n",
    "\n",
    "lda_no_pooling_coherence = CoherenceModel(topics=lda_topics_no_pooling, texts=tokenized_documents_no_pooling, dictionary=dictionary_no_pooling, window_size=10).get_coherence()\n",
    "lda_user_pooling_coherence = CoherenceModel(topics=lda_topics_user_pooling, texts=tokenized_documents_user_pooling, dictionary=dictionary_user_pooling, window_size=10).get_coherence()\n",
    "lda_hashtag_pooling_coherence = CoherenceModel(topics=lda_topics_hashtag_pooling, texts=tokenized_documents_hashtag_pooling, dictionary=dictionary_hashtag_pooling, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot a graph to compare coherence scores\n",
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align=\"center\", color=[\"red\",\"red\",\"red\",\"green\",\"green\",\"green\",\"blue\",\"blue\",\"blue\"])\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Coherence Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coherence scores\n",
    "evaluate_bar_graph([lsi_no_pooling_coherence, lsi_user_pooling_coherence, lsi_hashtag_pooling_coherence,\n",
    "                    hdp_no_pooling_coherence, hdp_user_pooling_coherence, hdp_hashtag_pooling_coherence,\n",
    "                    lda_no_pooling_coherence, lda_user_pooling_coherence, lda_hashtag_pooling_coherence],\n",
    "                   [\"LSI1\", \"LSI2\", \"LSI3\", \"HDP1\", \"HDP2\",\n",
    "                    \"HDP3\", \"LDA1\", \"LDA2\", \"LDA3\"])\n",
    "plt.savefig(\"topic_model_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDP models have the best coherence scores but the topics are way too granular on inspection. In fact, the LDA achieves similar coherence scores if the number of topics is increased. But to ensure human interpretability, the number of topics is restricted to a lower number. In addition, the HDP models are very unstable on retraining. The hashtag pooling LDA model has by far the highest coherence score of the LSI and LDA models which is in line with the result after human inspection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esade-topic-analysis-twitter",
   "language": "python",
   "name": "esade-topic-analysis-twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
